"""
SenseVoice è¯­éŸ³è¯†åˆ«å¼•æ“
åŸºäºé˜¿é‡Œè¾¾æ‘©é™¢çš„ SenseVoiceSmall æ¨¡å‹
æ”¯æŒï¼š
- å¤šè¯­è¨€è¯†åˆ«ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ï¼‰
- è¯´è¯äººåˆ†ç¦»ï¼ˆSpeaker Diarizationï¼ŒåŸºäº FunASR 1.2.7ï¼‰
- æƒ…æ„Ÿè¯†åˆ«
- æ—¶é—´æˆ³å¯¹é½

è¯´è¯äººåˆ†ç¦»å®ç°æ–¹æ¡ˆï¼š
ä½¿ç”¨ FunASR 1.2.7 è‡ªå¸¦çš„è¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼š
  1. FunASR CAM++ æ¨¡å‹è¿›è¡Œè¯´è¯äººåµŒå…¥æå–
  2. FunASR SOND æ¨¡å‹è¿›è¡Œè¯´è¯äººèšç±»åˆ†ç¦»
  3. SenseVoice å¯¹æ•´æ®µéŸ³é¢‘è¿›è¡Œè¯­éŸ³è¯†åˆ«
  4. æ ¹æ®è¯´è¯äººæ—¶é—´æˆ³åˆå¹¶è¯†åˆ«ç»“æœ

ä¼˜åŠ¿ï¼š
  - FunASR åŸç”Ÿæ”¯æŒï¼Œæ— éœ€é¢å¤–ä¾èµ–
  - ç»Ÿä¸€æ¡†æ¶ï¼Œå‡å°‘å…¼å®¹æ€§é—®é¢˜
  - æ€§èƒ½æ›´å¥½ï¼Œç»´æŠ¤æ›´ç®€å•
"""

import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from threading import Lock
from loguru import logger


class SenseVoiceEngine:
    """
    SenseVoice è¯­éŸ³è¯†åˆ«å¼•æ“ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    ç‰¹æ€§ï¼š
    - åŸºäº FunASR æ¡†æ¶
    - æ”¯æŒå¤šè¯­è¨€è‡ªåŠ¨è¯†åˆ«
    - æ”¯æŒè¯´è¯äººåˆ†ç¦»ï¼ˆåŸºäº FunASR 1.2.7 çš„åŸç”Ÿ speaker diarizationï¼‰
    - GPU åŠ é€Ÿ
    """

    _instance: Optional["SenseVoiceEngine"] = None
    _lock = Lock()
    _sensevoice_model = None  # SenseVoice æ¨¡å‹ï¼ˆä¸æ”¯æŒè¯´è¯äººåˆ†ç¦»ï¼‰
    _paraformer_model = None  # Paraformer æ¨¡å‹ï¼ˆæ”¯æŒè¯´è¯äººåˆ†ç¦»ï¼‰
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        model_dir: str = "iic/SenseVoiceSmall",
        cache_dir: Optional[str] = None,
        device: str = "cuda:0",
        enable_speaker_diarization: bool = False,
        use_paraformer_for_diarization: bool = False,
    ):
        """
        åˆå§‹åŒ– SenseVoice å¼•æ“ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰

        Args:
            model_dir: æ¨¡å‹è·¯å¾„æˆ–ModelScopeæ¨¡å‹ID
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            device: è®¾å¤‡ (cuda:0, cuda:1, cpu ç­‰)
            enable_speaker_diarization: æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»ï¼ˆé»˜è®¤å…³é—­ï¼Œéœ€è¦ä» API å‚æ•°æ§åˆ¶ï¼‰
            use_paraformer_for_diarization: æ˜¯å¦ä½¿ç”¨ Paraformer æ¨¡å‹è¿›è¡Œè¯´è¯äººåˆ†ç¦»ï¼ˆSenseVoice ä¸æ”¯æŒæ—¶é—´æˆ³ï¼‰
        """
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self.model_dir = model_dir
            self.device = device  # ä¿å­˜ device å‚æ•°
            self.enable_speaker_diarization = enable_speaker_diarization
            self.use_paraformer_for_diarization = use_paraformer_for_diarization

            # è¯´è¯äººåˆ†ç¦»è¯´æ˜
            if enable_speaker_diarization:
                logger.info("âš ï¸  è¯´è¯äººåˆ†ç¦»éœ€è¦æ—¶é—´æˆ³æ”¯æŒï¼Œå°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½ Paraformer æ¨¡å‹")

            # é»˜è®¤ç¼“å­˜ç›®å½•ï¼šé¡¹ç›®æ ¹ç›®å½•/models/sensevoice
            if cache_dir is None:
                project_root = Path(__file__).parent.parent.parent
                self.cache_dir = str(project_root / "models" / "sensevoice")
            else:
                self.cache_dir = cache_dir

            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            try:
                Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
            except (OSError, PermissionError) as e:
                # å¦‚æœä¸»ç¼“å­˜ç›®å½•æ— æ³•åˆ›å»ºï¼ˆå¦‚åªè¯»æ–‡ä»¶ç³»ç»Ÿï¼‰ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
                import tempfile

                self.cache_dir = str(Path(tempfile.gettempdir()) / "sensevoice_cache")
                logger.warning(f"âš ï¸  Failed to create cache directory, using temp: {self.cache_dir}")
                logger.warning(f"   Original error: {e}")
                try:
                    Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
                except Exception as e2:
                    logger.error(f"âŒ Failed to create temp cache directory: {e2}")
                    raise RuntimeError(f"Cannot create cache directory: {e2}") from e2

            self._initialized = True

            logger.info("ğŸ”§ SenseVoice Engine initialized")
            logger.info(f"   Model: {self.model_dir}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Cache: {self.cache_dir}")
            logger.info(
                f"   Speaker Diarization: {'âœ… Enabled (FunASR Native)' if enable_speaker_diarization else 'âŒ Disabled'}"
            )

    def _load_model(self, enable_sd: bool = False):
        """
        å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®éœ€æ±‚åŠ è½½ SenseVoice æˆ– Paraformerï¼‰

        Args:
            enable_sd: æ˜¯å¦éœ€è¦è¯´è¯äººåˆ†ç¦»æ”¯æŒ

        Returns:
            å¯¹åº”çš„æ¨¡å‹å®ä¾‹
        """
        with self._lock:
            logger.info("=" * 60)

            try:
                from funasr import AutoModel

                # å¦‚æœéœ€è¦è¯´è¯äººåˆ†ç¦»ï¼ŒåŠ è½½ Paraformer æ¨¡å‹
                if enable_sd:
                    if self._paraformer_model is None:
                        logger.info("ğŸ“¥ Loading Paraformer Model (with Speaker Diarization)...")
                        logger.info("=" * 60)
                        logger.info("ğŸ¤– Loading Paraformer + CAM++ + Punc models...")

                        # ä½¿ç”¨ Paraformer æ¨¡å‹ï¼ˆæ”¯æŒæ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»ï¼‰
                        # å‚è€ƒï¼šhttps://github.com/lukeewin/AudioSeparationGUI
                        self._paraformer_model = AutoModel(
                            model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                            vad_model="fsmn-vad",
                            vad_kwargs={"max_single_segment_time": 30000},
                            punc_model="ct-punc",  # æ ‡ç‚¹æ¨¡å‹ï¼ˆè¯´è¯äººåˆ†ç¦»å¿…éœ€ï¼‰
                            spk_model="iic/speech_campplus_sv_zh-cn_16k-common",  # è¯´è¯äººåµŒå…¥æ¨¡å‹
                            device=self.device,
                        )

                        logger.info("=" * 60)
                        logger.info("âœ… Paraformer Model loaded successfully!")
                        logger.info("   Features:")
                        logger.info("   - Multi-language ASR (zh-cn)")
                        logger.info("   - Timestamp Support âœ…")
                        logger.info("   - Speaker Diarization (CAM++) âœ…")
                        logger.info("   - Punctuation Prediction")
                        logger.info("=" * 60)
                    else:
                        logger.debug("â™»ï¸  Using cached Paraformer model")

                    return self._paraformer_model

                else:
                    # ä¸éœ€è¦è¯´è¯äººåˆ†ç¦»ï¼Œä½¿ç”¨ SenseVoice æ¨¡å‹
                    if self._sensevoice_model is None:
                        logger.info("ğŸ“¥ Loading SenseVoice Model...")
                        logger.info("=" * 60)
                        logger.info(f"ğŸ¤– Loading model from: {self.model_dir}")

                        self._sensevoice_model = AutoModel(
                            model=self.model_dir,
                            trust_remote_code=True,
                            remote_code="./model.py",
                            vad_model="fsmn-vad",
                            vad_kwargs={"max_single_segment_time": 30000},
                            device=self.device,
                        )

                        logger.info("=" * 60)
                        logger.info("âœ… SenseVoice Model loaded successfully!")
                        logger.info("   Features:")
                        logger.info("   - Multi-language ASR (zh/en/ja/ko/yue)")
                        logger.info("   - Emotion Recognition")
                        logger.info("=" * 60)
                    else:
                        logger.debug("â™»ï¸  Using cached SenseVoice model")

                    return self._sensevoice_model

            except Exception as e:
                logger.error("=" * 80)
                logger.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥:")
                logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
                logger.error("")
                logger.error("ğŸ’¡ æ’æŸ¥å»ºè®®:")
                logger.error("   1. å®‰è£… FunASR:")
                logger.error("      pip install funasr>=1.2.7")
                logger.error("   2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰")
                logger.error("   3. æ£€æŸ¥ GPU å¯ç”¨æ€§")
                logger.error("   4. æ¨¡å‹ä¼šè‡ªåŠ¨ä» ModelScope ä¸‹è½½")
                logger.error("=" * 80)

                import traceback

                logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
                logger.debug(traceback.format_exc())

                raise

    def parse(
        self, audio_path: str, output_path: str, language: str = "auto", use_itn: bool = True, **kwargs
    ) -> Dict[str, Any]:
        """
        è¯­éŸ³è¯†åˆ«ï¼ˆæ”¯æŒ FunASR åŸç”Ÿè¯´è¯äººåˆ†ç¦»ï¼‰

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºç›®å½•
            language: è¯­è¨€ä»£ç  (auto/zh/en/ja/ko/yue)
            use_itn: æ˜¯å¦ä½¿ç”¨é€†æ–‡æœ¬å½’ä¸€åŒ–ï¼ˆæ•°å­—ã€æ—¥æœŸç­‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
                - enable_speaker_diarization: æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»ï¼ˆè¦†ç›–åˆå§‹åŒ–è®¾ç½®ï¼‰

        Returns:
            è§£æç»“æœï¼ˆJSONæ ¼å¼ï¼ŒåŒ…å«è¯´è¯äººä¿¡æ¯ï¼‰
        """
        audio_path = Path(audio_path)
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
        enable_sd = kwargs.get("enable_speaker_diarization", self.enable_speaker_diarization)

        logger.info(f"ğŸ™ï¸  SenseVoice processing: {audio_path.name}")
        logger.info(f"   Language: {language}")
        logger.info(f"   Speaker Diarization: {'âœ… Enabled (Paraformer)' if enable_sd else 'âŒ Disabled (SenseVoice)'}")

        # åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ® enable_sd å†³å®šåŠ è½½ SenseVoice è¿˜æ˜¯ Paraformerï¼‰
        model = self._load_model(enable_sd=enable_sd)

        # æ‰§è¡Œæ¨ç†
        try:
            logger.info("ğŸš€ å¼€å§‹è¯­éŸ³è¯†åˆ«...")

            # å¦‚æœå¯ç”¨è¯´è¯äººåˆ†ç¦»ï¼Œä½¿ç”¨ Paraformer + CAM++
            if enable_sd:
                logger.info("ğŸ¯ ä½¿ç”¨ Paraformer è¯´è¯äººåˆ†ç¦»æ¨¡å¼...")
                parsed_result = self._parse_with_funasr_diarization(audio_path, model, None, language, use_itn)
            else:
                logger.info("ğŸ¯ ä½¿ç”¨ SenseVoice åŸºç¡€è¯†åˆ«æ¨¡å¼...")
                # FunASR æ¨ç†ï¼ˆåŸºç¡€æ¨¡å¼ï¼‰
                result = model.generate(
                    input=str(audio_path),
                    cache={},
                    language=language,
                    use_itn=use_itn,
                    batch_size=60,
                    merge_vad=True,  # åˆå¹¶VADç»“æœ
                    merge_length_s=15,  # åˆå¹¶é•¿åº¦
                )

                logger.info("âœ… SenseVoice completed")

                # è§£æç»“æœ
                parsed_result = self._parse_result(result, audio_path)

            # ç”Ÿæˆ Markdown
            markdown_content = self._generate_markdown(parsed_result)

            # ä¿å­˜ä¸ºç»Ÿä¸€çš„ content.mdï¼ˆä¸»ç»“æœï¼‰
            content_md_file = output_path / "content.md"
            content_md_file.write_text(markdown_content, encoding="utf-8")
            logger.info("ğŸ“„ Main result saved: content.md")

            # åŒæ—¶ä¿ç•™åŸå§‹å‘½åçš„æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•/å¤‡ä»½ï¼‰
            original_md_file = output_path / f"{audio_path.stem}.md"
            original_md_file.write_text(markdown_content, encoding="utf-8")
            logger.info(f"ğŸ“„ Backup saved: {original_md_file.name}")

            # ä¿å­˜ä¸ºç»Ÿä¸€çš„ content.jsonï¼ˆä¸»ç»“æœï¼‰
            content_json_file = output_path / "content.json"
            with open(content_json_file, "w", encoding="utf-8") as f:
                json.dump(parsed_result, f, ensure_ascii=False, indent=2)
            logger.info("ğŸ“„ Main JSON saved: content.json")

            # åŒæ—¶ä¿ç•™åŸå§‹å‘½åçš„æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•/å¤‡ä»½ï¼‰
            original_json_file = output_path / f"{audio_path.stem}.json"
            with open(original_json_file, "w", encoding="utf-8") as f:
                json.dump(parsed_result, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ Backup JSON saved: {original_json_file.name}")

            return {
                "success": True,
                "output_path": str(output_path),
                "markdown": markdown_content,
                "markdown_file": str(content_md_file),
                "json_file": str(content_json_file),
                "json_data": parsed_result,
            }

        except Exception as e:
            logger.error("=" * 80)
            logger.error("âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥:")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            logger.error("=" * 80)

            import traceback

            logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            logger.debug(traceback.format_exc())

            raise

    def _parse_with_funasr_diarization(
        self, audio_path: Path, asr_model, sd_model, language: str, use_itn: bool
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ FunASR åŸç”Ÿè¯´è¯äººåˆ†ç¦» + SenseVoice è¿›è¡Œè¯­éŸ³è¯†åˆ«

        æµç¨‹ï¼š
        1. ä½¿ç”¨ FunASR çš„ spk_mode å‚æ•°å¯ç”¨è¯´è¯äººåˆ†ç¦»
        2. SenseVoice å¯¹éŸ³é¢‘è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼ŒåŒæ—¶ CAM++ æ¨¡å‹æå–è¯´è¯äººç‰¹å¾
        3. FunASR è‡ªåŠ¨ä¸ºæ¯ä¸ªè¯­éŸ³æ®µåˆ†é…è¯´è¯äººæ ‡ç­¾
        4. è§£æç»“æœå¹¶ä¿ç•™è¯´è¯äººä¿¡æ¯

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            asr_model: SenseVoice ASR æ¨¡å‹ï¼ˆå·²åŠ è½½ CAM++ è¯´è¯äººæ¨¡å‹ï¼‰
            sd_model: è¯´è¯äººåˆ†ç¦»æ¨¡å‹ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
            language: è¯­è¨€ä»£ç 
            use_itn: æ˜¯å¦ä½¿ç”¨é€†æ–‡æœ¬å½’ä¸€åŒ–

        Returns:
            æ ‡å‡†åŒ–çš„ JSON ç»“æœ
        """
        try:
            # ä½¿ç”¨ FunASR Paraformer + CAM++ è¿›è¡Œè¯´è¯äººåˆ†ç¦»
            logger.info("   [1/2] Paraformer è¿›è¡Œè¯­éŸ³è¯†åˆ« + è¯´è¯äººåˆ†ç¦»...")

            # å…³é”®å‚æ•°ï¼š
            # - batch_size_s: æ‰¹å¤„ç†å¤§å°ï¼ˆç§’ï¼‰
            # - sentence_timestamp: å¯ç”¨å¥å­çº§æ—¶é—´æˆ³ï¼ˆè¯´è¯äººåˆ†ç¦»å¿…éœ€ï¼‰
            asr_result = asr_model.generate(
                input=str(audio_path),
                batch_size_s=300,  # æ‰¹å¤„ç†å¤§å°ï¼ˆç§’ï¼‰
                sentence_timestamp=True,  # å¯ç”¨å¥å­çº§æ—¶é—´æˆ³ï¼ˆå…³é”®ï¼ï¼‰
                is_final=True,  # æœ€ç»ˆç»“æœ
            )

            logger.info("   [2/2] è§£æè¯´è¯äººåˆ†ç¦»ç»“æœ...")

            # è§£æ Paraformer ç»“æœï¼ˆåŒ…å« sentence_info å’Œè¯´è¯äººä¿¡æ¯ï¼‰
            parsed_result = self._parse_paraformer_result(asr_result, audio_path)

            # ç»Ÿè®¡è¯´è¯äººæ•°é‡
            segments = parsed_result.get("segments", [])
            speakers = set(seg.get("speaker", "SPEAKER_00") for seg in segments if seg.get("speaker"))
            speaker_count = len(speakers) if speakers else 1

            logger.info(f"âœ… è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œæ£€æµ‹åˆ° {speaker_count} ä½è¯´è¯äººï¼ˆParaformer + CAM++ï¼‰")

            parsed_result["metadata"]["speaker_diarization_enabled"] = True
            parsed_result["metadata"]["speaker_diarization_method"] = "Paraformer + CAM++ (sentence-level)"
            parsed_result["metadata"]["speaker_count"] = speaker_count

            return parsed_result

        except Exception as e:
            logger.error(f"âŒ FunASR è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
            logger.warning("âš ï¸  å›é€€åˆ°åŸºç¡€è¯†åˆ«æ¨¡å¼...")

            import traceback

            logger.debug(traceback.format_exc())

            # å›é€€åˆ°åŸºç¡€æ¨¡å¼ï¼ˆä¸ä½¿ç”¨è¯´è¯äººåˆ†ç¦»ï¼‰
            result = asr_model.generate(
                input=str(audio_path),
                cache={},
                language=language,
                use_itn=use_itn,
                batch_size=60,
                merge_vad=True,
                merge_length_s=15,
            )

            return self._parse_result(result, audio_path)

    def _parse_paraformer_result(self, result: List[Dict], audio_path: Path) -> Dict[str, Any]:
        """
        è§£æ Paraformer çš„ç»“æœï¼ˆåŒ…å« sentence_info å’Œè¯´è¯äººä¿¡æ¯ï¼‰

        Paraformer è¿”å›æ ¼å¼ï¼š
        {
            'text': 'å®Œæ•´æ–‡æœ¬',
            'sentence_info': [
                {'text': 'å¥å­1', 'start': 0, 'end': 1000, 'spk': 0},
                {'text': 'å¥å­2', 'start': 1000, 'end': 2000, 'spk': 1},
                ...
            ]
        }

        Args:
            result: Paraformer è¿”å›çš„ç»“æœåˆ—è¡¨
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ ‡å‡†åŒ–çš„ JSON ç»“æœï¼ˆåŒ…å«è¯´è¯äººä¿¡æ¯ï¼‰
        """
        if not result or len(result) == 0:
            return {
                "format": "audio",
                "audio_file": audio_path.name,
                "transcript": "",
                "segments": [],
                "metadata": {
                    "duration": 0,
                    "language": "zh-cn",
                    "speaker_count": 0,
                },
            }

        # è·å–ç¬¬ä¸€ä¸ªç»“æœ
        first_result = result[0]

        # æå–å®Œæ•´æ–‡æœ¬
        transcript = first_result.get("text", "")

        # æå–å¥å­çº§ä¿¡æ¯ï¼ˆåŒ…å«è¯´è¯äººæ ‡ç­¾ï¼‰
        sentence_info = first_result.get("sentence_info", [])

        # æ„å»ºè¯­éŸ³æ®µåˆ—è¡¨
        segments = []
        for sentence in sentence_info:
            text = sentence.get("text", "")
            start_ms = sentence.get("start", 0)  # æ¯«ç§’
            end_ms = sentence.get("end", 0)  # æ¯«ç§’
            spk = sentence.get("spk", 0)  # è¯´è¯äºº IDï¼ˆæ•´æ•°ï¼‰

            # æ ¼å¼åŒ–è¯´è¯äººæ ‡ç­¾
            speaker = f"SPEAKER_{spk:02d}"

            segments.append(
                {
                    "start": start_ms / 1000.0,  # è½¬æ¢ä¸ºç§’
                    "end": end_ms / 1000.0,
                    "text": text,
                    "speaker": speaker,
                    "language": "zh-cn",  # Paraformer ä¸»è¦æ”¯æŒä¸­æ–‡
                    "emotion": "neutral",  # Paraformer ä¸æ”¯æŒæƒ…æ„Ÿè¯†åˆ«
                }
            )

        # è®¡ç®—æ€»æ—¶é•¿
        duration = segments[-1]["end"] if segments else 0

        # ç»Ÿè®¡è¯´è¯äººæ•°é‡
        speakers = set(seg.get("speaker", "SPEAKER_00") for seg in segments)
        speaker_count = len(speakers)

        return {
            "format": "audio",
            "audio_file": audio_path.name,
            "transcript": transcript,
            "segments": segments,
            "metadata": {
                "duration": duration,
                "language": "zh-cn",
                "speaker_count": speaker_count,
                "emotion_enabled": False,  # Paraformer ä¸æ”¯æŒæƒ…æ„Ÿ
            },
        }

    def _parse_result_with_speaker(self, result: List[Dict], audio_path: Path) -> Dict[str, Any]:
        """
        è§£æ FunASR çš„åŸå§‹ç»“æœä¸ºæ ‡å‡†åŒ–æ ¼å¼ï¼ˆåŒ…å«è¯´è¯äººä¿¡æ¯ï¼‰

        Args:
            result: FunASR è¿”å›çš„ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«è¯´è¯äººæ ‡ç­¾ï¼‰
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ ‡å‡†åŒ–çš„ JSON ç»“æœï¼ˆåŒ…å«è¯´è¯äººä¿¡æ¯ï¼‰
        """
        if not result or len(result) == 0:
            return {
                "format": "audio",
                "audio_file": audio_path.name,
                "transcript": "",
                "segments": [],
                "metadata": {
                    "duration": 0,
                    "language": "unknown",
                    "speaker_count": 0,
                },
            }

        # è·å–ç¬¬ä¸€ä¸ªç»“æœï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
        first_result = result[0]

        # æå–æ–‡æœ¬
        transcript = first_result.get("text", "")

        # æå–æ—¶é—´æˆ³ï¼ˆå¦‚æœæœ‰ï¼‰
        timestamp = first_result.get("timestamp", [])

        # æå–è¯­è¨€æ ‡ç­¾
        language_tags = first_result.get("language", [])

        # æå–æƒ…æ„Ÿæ ‡ç­¾
        emotion_tags = first_result.get("emotion", [])

        # æå–è¯´è¯äººæ ‡ç­¾ï¼ˆFunASR è¿”å›çš„è¯´è¯äºº IDï¼‰
        speaker_tags = first_result.get("spk", [])  # æˆ–è€… "speaker"ï¼Œå–å†³äº FunASR ç‰ˆæœ¬

        # æ„å»ºè¯­éŸ³æ®µåˆ—è¡¨
        segments = []
        if timestamp:
            # timestamp æ ¼å¼: [[word_index, start_ms, end_ms], ...]
            for i, ts in enumerate(timestamp):
                # ts å¯èƒ½æ˜¯ [start, end] æˆ– [word_idx, start, end]
                if len(ts) == 3:
                    word_idx, start_ms, end_ms = ts
                elif len(ts) == 2:
                    start_ms, end_ms = ts
                    word_idx = i
                else:
                    continue

                # ä»å®Œæ•´æ–‡æœ¬ä¸­æå–å¯¹åº”çš„è¯ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                words = transcript.split()
                if word_idx < len(words):
                    text = words[word_idx]
                else:
                    text = ""

                # è·å–è¯­è¨€å’Œæƒ…æ„Ÿï¼ˆå¦‚æœæœ‰ï¼‰
                lang = language_tags[i] if i < len(language_tags) else "unknown"
                emotion = emotion_tags[i] if i < len(emotion_tags) else "neutral"

                # è·å–è¯´è¯äººæ ‡ç­¾ï¼ˆå…³é”®ï¼šä» FunASR ç»“æœä¸­æå–ï¼‰
                speaker = speaker_tags[i] if i < len(speaker_tags) else "SPEAKER_00"
                # æ ¼å¼åŒ–è¯´è¯äººæ ‡ç­¾
                if isinstance(speaker, int):
                    speaker = f"SPEAKER_{speaker:02d}"
                elif not isinstance(speaker, str):
                    speaker = f"SPEAKER_{str(speaker)}"

                segments.append(
                    {
                        "start": start_ms / 1000.0,  # è½¬æ¢ä¸ºç§’
                        "end": end_ms / 1000.0,
                        "text": text,
                        "language": lang,
                        "emotion": emotion,
                        "speaker": speaker,  # æ·»åŠ è¯´è¯äººæ ‡ç­¾
                    }
                )
        else:
            # æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªå•ä¸€æ®µè½
            speaker = speaker_tags[0] if speaker_tags else "SPEAKER_00"
            if isinstance(speaker, int):
                speaker = f"SPEAKER_{speaker:02d}"

            segments.append(
                {
                    "start": 0,
                    "end": 0,
                    "text": transcript,
                    "language": language_tags[0] if language_tags else "unknown",
                    "emotion": emotion_tags[0] if emotion_tags else "neutral",
                    "speaker": speaker,
                }
            )

        # è®¡ç®—æ€»æ—¶é•¿
        duration = segments[-1]["end"] if segments else 0

        # æ£€æµ‹è¯­è¨€
        detected_language = language_tags[0] if language_tags else "auto"

        # ç»Ÿè®¡è¯´è¯äººæ•°é‡
        speakers = set(seg.get("speaker", "SPEAKER_00") for seg in segments)
        speaker_count = len(speakers)

        return {
            "format": "audio",
            "audio_file": audio_path.name,
            "transcript": transcript,
            "segments": segments,
            "metadata": {
                "duration": duration,
                "language": detected_language,
                "speaker_count": speaker_count,
                "emotion_enabled": bool(emotion_tags),
            },
        }

    def _parse_result(self, result: List[Dict], audio_path: Path) -> Dict[str, Any]:
        """
        è§£æ FunASR çš„åŸå§‹ç»“æœä¸ºæ ‡å‡†åŒ–æ ¼å¼

        Args:
            result: FunASR è¿”å›çš„ç»“æœåˆ—è¡¨
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ ‡å‡†åŒ–çš„ JSON ç»“æœ
        """
        if not result or len(result) == 0:
            return {
                "format": "audio",
                "audio_file": audio_path.name,
                "transcript": "",
                "segments": [],
                "metadata": {
                    "duration": 0,
                    "language": "unknown",
                    "speaker_count": 0,
                },
            }

        # è·å–ç¬¬ä¸€ä¸ªç»“æœï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
        first_result = result[0]

        # æå–æ–‡æœ¬
        transcript = first_result.get("text", "")

        # æå–æ—¶é—´æˆ³ï¼ˆå¦‚æœæœ‰ï¼‰
        timestamp = first_result.get("timestamp", [])

        # æå–è¯­è¨€æ ‡ç­¾
        language_tags = first_result.get("language", [])

        # æå–æƒ…æ„Ÿæ ‡ç­¾
        emotion_tags = first_result.get("emotion", [])

        # æ„å»ºè¯­éŸ³æ®µåˆ—è¡¨
        segments = []
        if timestamp:
            # timestamp æ ¼å¼: [[word_index, start_ms, end_ms], ...]
            for i, ts in enumerate(timestamp):
                # ts å¯èƒ½æ˜¯ [start, end] æˆ– [word_idx, start, end]
                if len(ts) == 3:
                    word_idx, start_ms, end_ms = ts
                elif len(ts) == 2:
                    start_ms, end_ms = ts
                    word_idx = i
                else:
                    continue

                # ä»å®Œæ•´æ–‡æœ¬ä¸­æå–å¯¹åº”çš„è¯ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                words = transcript.split()
                if word_idx < len(words):
                    text = words[word_idx]
                else:
                    text = ""

                # è·å–è¯­è¨€å’Œæƒ…æ„Ÿï¼ˆå¦‚æœæœ‰ï¼‰
                lang = language_tags[i] if i < len(language_tags) else "unknown"
                emotion = emotion_tags[i] if i < len(emotion_tags) else "neutral"

                segments.append(
                    {
                        "start": start_ms / 1000.0,  # è½¬æ¢ä¸ºç§’
                        "end": end_ms / 1000.0,
                        "text": text,
                        "language": lang,
                        "emotion": emotion,
                    }
                )
        else:
            # æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªå•ä¸€æ®µè½
            segments.append(
                {
                    "start": 0,
                    "end": 0,
                    "text": transcript,
                    "language": language_tags[0] if language_tags else "unknown",
                    "emotion": emotion_tags[0] if emotion_tags else "neutral",
                }
            )

        # è®¡ç®—æ€»æ—¶é•¿
        duration = segments[-1]["end"] if segments else 0

        # æ£€æµ‹è¯­è¨€
        detected_language = language_tags[0] if language_tags else "auto"

        return {
            "format": "audio",
            "audio_file": audio_path.name,
            "transcript": transcript,
            "segments": segments,
            "metadata": {
                "duration": duration,
                "language": detected_language,
                "speaker_count": 1,  # åŸºç¡€æ¨¡å¼é»˜è®¤ä¸º1ä¸ªè¯´è¯äºº
                "emotion_enabled": bool(emotion_tags),
            },
        }

    def _generate_markdown(self, parsed_result: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„è½¬å½•æ–‡æœ¬

        Args:
            parsed_result: æ ‡å‡†åŒ–çš„ JSON ç»“æœ

        Returns:
            Markdown æ ¼å¼çš„æ–‡æœ¬
        """
        lines = []

        # æ ‡é¢˜
        lines.append("# éŸ³é¢‘è½¬å½•ç»“æœ\n")
        lines.append(f"**æ–‡ä»¶**: {parsed_result['audio_file']}\n")

        # å…ƒæ•°æ®
        metadata = parsed_result.get("metadata", {})
        lines.append("## å…ƒæ•°æ®\n")
        lines.append(f"- **æ—¶é•¿**: {metadata.get('duration', 0):.2f} ç§’")
        lines.append(f"- **è¯­è¨€**: {metadata.get('language', 'unknown')}")
        lines.append(f"- **è¯´è¯äººæ•°é‡**: {metadata.get('speaker_count', 1)}")

        if metadata.get("speaker_diarization_enabled"):
            lines.append(f"- **è¯´è¯äººåˆ†ç¦»**: âœ… å·²å¯ç”¨ ({metadata.get('speaker_diarization_method', 'FunASR')})")

        if metadata.get("emotion_enabled"):
            lines.append("- **æƒ…æ„Ÿè¯†åˆ«**: âœ… å·²å¯ç”¨")

        lines.append("")

        # å®Œæ•´è½¬å½•æ–‡æœ¬
        lines.append("## å®Œæ•´è½¬å½•\n")
        lines.append(parsed_result.get("transcript", ""))
        lines.append("")

        # è¯¦ç»†æ—¶é—´æˆ³ï¼ˆä»…å½“æœ‰æœ‰æ•ˆæ—¶é—´æˆ³æ—¶æ‰æ˜¾ç¤ºï¼‰
        segments = parsed_result.get("segments", [])
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ—¶é—´æˆ³ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ª segment çš„ start æˆ– end ä¸ä¸º 0ï¼‰
        has_valid_timestamps = any(seg.get("start", 0) != 0 or seg.get("end", 0) != 0 for seg in segments)

        if segments and has_valid_timestamps:
            lines.append("## è¯¦ç»†æ—¶é—´æˆ³\n")

            for seg in segments:
                start = seg.get("start", 0)
                end = seg.get("end", 0)
                text = seg.get("text", "")
                speaker = seg.get("speaker", "")
                emotion = seg.get("emotion", "")

                # æ ¼å¼åŒ–æ—¶é—´æˆ³
                timestamp = f"[{self._format_time(start)} --> {self._format_time(end)}]"

                # æ·»åŠ è¯´è¯äººæ ‡ç­¾
                speaker_tag = f"**{speaker}**: " if speaker else ""

                # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾
                emotion_tag = f" *({emotion})*" if emotion and emotion != "neutral" else ""

                lines.append(f"{timestamp} {speaker_tag}{text}{emotion_tag}")

            lines.append("")

        return "\n".join(lines)

    def _format_time(self, seconds: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´ï¼ˆç§’ -> HH:MM:SS.mmmï¼‰

        Args:
            seconds: ç§’æ•°

        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60

        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"
        else:
            return f"{minutes:02d}:{secs:06.3f}"
