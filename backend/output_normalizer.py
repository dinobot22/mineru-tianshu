"""
è¾“å‡ºç»“æœè§„èŒƒåŒ–æ¨¡å—

ç»Ÿä¸€ä¸åŒè§£æå¼•æ“çš„è¾“å‡ºæ ¼å¼ï¼Œç¡®ä¿ï¼š
1. Markdown æ–‡ä»¶åç»Ÿä¸€ä¸º result.md
2. å›¾ç‰‡ç›®å½•ç»Ÿä¸€ä¸º images/
3. å›¾ç‰‡å¼•ç”¨è·¯å¾„ç»Ÿä¸€ä¸º images/xxx.jpg
4. JSON æ–‡ä»¶åç»Ÿä¸€ä¸º result.json
5. è‡ªåŠ¨ä¸Šä¼ å›¾ç‰‡åˆ° RustFS å¯¹è±¡å­˜å‚¨å¹¶æ›¿æ¢ URL

æ”¯æŒçš„å¼•æ“ï¼š
- MinerU (pipeline)
- PaddleOCR-VL
- SenseVoice
- Video Processing
- Format Engines (FASTA, GenBank, etc.)
"""

from pathlib import Path
from typing import Optional, Dict, Any
from loguru import logger
import shutil
import re
import json


class OutputNormalizer:
    """
    è¾“å‡ºç»“æœè§„èŒƒåŒ–å™¨

    å°†ä¸åŒå¼•æ“çš„è¾“å‡ºç»Ÿä¸€ä¸ºæ ‡å‡†æ ¼å¼ï¼š
    - result.md: ä¸» Markdown æ–‡ä»¶
    - images/: å›¾ç‰‡ç›®å½•ï¼ˆç»Ÿä¸€åç§°ï¼‰
    - result.json: ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    """

    # æ ‡å‡†è¾“å‡ºæ–‡ä»¶å
    STANDARD_MARKDOWN_NAME = "result.md"
    STANDARD_JSON_NAME = "result.json"
    STANDARD_IMAGE_DIR = "images"

    def __init__(self, output_dir: Path):
        """
        åˆå§‹åŒ–è§„èŒƒåŒ–å™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¼•æ“çš„åŸå§‹è¾“å‡ºç›®å½•ï¼‰

        æ³¨æ„ï¼šRustFS è‡ªåŠ¨ä¸Šä¼ å·²é›†æˆä¸ºåŸºç¡€åŠŸèƒ½ï¼Œå§‹ç»ˆå¯ç”¨
        """
        self.output_dir = Path(output_dir)
        if not self.output_dir.exists():
            raise ValueError(f"Output directory does not exist: {output_dir}")

        self._rustfs_client = None

    def normalize(self) -> Dict[str, Any]:
        """
        è§„èŒƒåŒ–è¾“å‡ºç›®å½•

        Returns:
            è§„èŒƒåŒ–åçš„æ–‡ä»¶ä¿¡æ¯
        """
        logger.info(f"ğŸ”§ Normalizing output directory: {self.output_dir}")

        result = {
            "markdown_file": None,
            "json_file": None,
            "image_dir": None,
            "image_count": 0,
            "rustfs_enabled": False,
            "images_uploaded": False,
        }

        # 1. è§„èŒƒåŒ– Markdown æ–‡ä»¶
        result["markdown_file"] = self._normalize_markdown()

        # 2. è§„èŒƒåŒ–å›¾ç‰‡ç›®å½•
        result["image_dir"], result["image_count"] = self._normalize_images()

        # 3. è§„èŒƒåŒ– JSON æ–‡ä»¶
        result["json_file"] = self._normalize_json()

        # 4. å¦‚æœæœ‰å›¾ç‰‡ç›®å½•ï¼Œæ›´æ–° Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨
        if result["image_dir"] and result["markdown_file"]:
            self._update_markdown_image_refs(result["markdown_file"])

        # 5. è‡ªåŠ¨ä¸Šä¼ å›¾ç‰‡åˆ° RustFS å¹¶æ›¿æ¢ URLï¼ˆåŸºç¡€åŠŸèƒ½ï¼Œå§‹ç»ˆå¯ç”¨ï¼‰
        if result["image_dir"] and result["image_count"] > 0:
            try:
                logger.info(f"ğŸ“¤ Uploading {result['image_count']} images to RustFS...")
                url_mapping = self._upload_images_to_rustfs(result["image_dir"])

                if url_mapping:
                    # æ›¿æ¢ Markdown ä¸­çš„å›¾ç‰‡è·¯å¾„
                    if result["markdown_file"]:
                        self._replace_markdown_urls(result["markdown_file"], url_mapping)

                    # æ›¿æ¢ JSON ä¸­çš„å›¾ç‰‡è·¯å¾„
                    if result["json_file"]:
                        self._replace_json_urls(result["json_file"], url_mapping)

                    result["rustfs_enabled"] = True
                    result["images_uploaded"] = True
                    logger.info(f"âœ… Images uploaded to RustFS: {len(url_mapping)}/{result['image_count']}")
                else:
                    logger.warning("âš ï¸  No images uploaded (url_mapping empty)")
                    result["rustfs_enabled"] = False
                    result["images_uploaded"] = False
            except Exception as e:
                logger.error(f"âŒ Failed to upload images to RustFS: {e}")
                logger.error(f"   Error details: {type(e).__name__}: {str(e)}")
                result["rustfs_enabled"] = False
                result["images_uploaded"] = False
                # RustFS ä¸Šä¼ å¤±è´¥ä¸åº”ä¸­æ–­ä¸»æµç¨‹ï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°è·¯å¾„
                logger.warning("âš ï¸  Continuing with local image paths (RustFS upload failed)")
        else:
            logger.debug("â„¹ï¸  No images to upload")
            result["rustfs_enabled"] = False
            result["images_uploaded"] = False

        logger.info("âœ… Normalization complete:")
        logger.info(f"   Markdown: {result['markdown_file']}")
        logger.info(f"   Images: {result['image_count']} files in {result['image_dir']}")
        logger.info(f"   JSON: {result['json_file']}")
        logger.info(f"   RustFS: {result['rustfs_enabled']} (uploaded: {result['images_uploaded']})")

        return result

    def _normalize_markdown(self) -> Optional[Path]:
        """
        è§„èŒƒåŒ– Markdown æ–‡ä»¶

        æŸ¥æ‰¾å¹¶é‡å‘½åä¸ºæ ‡å‡†åç§°ï¼šresult.md
        """
        # æŸ¥æ‰¾æ‰€æœ‰ .md æ–‡ä»¶ï¼ˆé€’å½’ï¼‰
        md_files = list(self.output_dir.rglob("*.md"))

        if not md_files:
            logger.warning("âš ï¸  No markdown files found")
            return None

        # å¦‚æœå·²ç»æœ‰ result.mdï¼Œç›´æ¥è¿”å›
        standard_md = self.output_dir / self.STANDARD_MARKDOWN_NAME
        if standard_md.exists():
            logger.info(f"âœ… Standard markdown file already exists: {standard_md.name}")
            return standard_md

        # é€‰æ‹©æœ€å¤§çš„ .md æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ä¸»æ–‡ä»¶ï¼‰
        main_md = max(md_files, key=lambda f: f.stat().st_size)
        logger.info(f"ğŸ“„ Found main markdown: {main_md.relative_to(self.output_dir)}")

        # å¦‚æœä¸åœ¨æ ¹ç›®å½•ï¼Œç§»åŠ¨åˆ°æ ¹ç›®å½•
        if main_md.parent != self.output_dir:
            logger.info("   Moving to root directory...")
            shutil.copy2(main_md, standard_md)
        else:
            # é‡å‘½å
            logger.info(f"   Renaming to {self.STANDARD_MARKDOWN_NAME}...")
            main_md.rename(standard_md)

        return standard_md

    def _normalize_images(self) -> tuple[Optional[Path], int]:
        """
        è§„èŒƒåŒ–å›¾ç‰‡ç›®å½•

        å°†æ‰€æœ‰å›¾ç‰‡ç»Ÿä¸€åˆ° images/ ç›®å½•
        """
        standard_image_dir = self.output_dir / self.STANDARD_IMAGE_DIR

        # æŸ¥æ‰¾å¯èƒ½çš„å›¾ç‰‡ç›®å½•
        possible_dirs = ["imgs", "images", "img", "pictures", "pics"]
        found_dirs = []

        for dir_name in possible_dirs:
            img_dir = self.output_dir / dir_name
            if img_dir.exists() and img_dir.is_dir():
                found_dirs.append(img_dir)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ç›®å½•ï¼ŒæŸ¥æ‰¾æ•£è½çš„å›¾ç‰‡æ–‡ä»¶
        if not found_dirs:
            image_extensions = {".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".svg"}
            image_files = [
                f for f in self.output_dir.rglob("*") if f.is_file() and f.suffix.lower() in image_extensions
            ]

            if not image_files:
                logger.info("â„¹ï¸  No images found")
                return None, 0

            # åˆ›å»ºæ ‡å‡†å›¾ç‰‡ç›®å½•å¹¶ç§»åŠ¨å›¾ç‰‡
            logger.info(f"ğŸ“ Creating standard image directory: {self.STANDARD_IMAGE_DIR}/")
            standard_image_dir.mkdir(exist_ok=True)

            for img_file in image_files:
                if img_file.parent != standard_image_dir:
                    dest = standard_image_dir / img_file.name
                    logger.debug(f"   Moving: {img_file.name}")
                    shutil.move(str(img_file), str(dest))

            return standard_image_dir, len(image_files)

        # å¦‚æœæ ‡å‡†ç›®å½•å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if standard_image_dir in found_dirs:
            image_count = len(list(standard_image_dir.iterdir()))
            logger.info(f"âœ… Standard image directory already exists: {self.STANDARD_IMAGE_DIR}/")
            return standard_image_dir, image_count

        # åˆå¹¶æ‰€æœ‰å›¾ç‰‡ç›®å½•åˆ°æ ‡å‡†ç›®å½•
        logger.info(f"ğŸ“ Consolidating image directories to: {self.STANDARD_IMAGE_DIR}/")
        standard_image_dir.mkdir(exist_ok=True)

        total_images = 0
        for img_dir in found_dirs:
            logger.info(f"   Processing: {img_dir.name}/")
            for img_file in img_dir.iterdir():
                if img_file.is_file():
                    dest = standard_image_dir / img_file.name
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    if dest.exists():
                        stem = img_file.stem
                        suffix = img_file.suffix
                        counter = 1
                        while dest.exists():
                            dest = standard_image_dir / f"{stem}_{counter}{suffix}"
                            counter += 1

                    shutil.move(str(img_file), str(dest))
                    total_images += 1

            # åˆ é™¤ç©ºç›®å½•
            try:
                img_dir.rmdir()
                logger.debug(f"   Removed empty directory: {img_dir.name}/")
            except OSError:
                pass

        return standard_image_dir, total_images

    def _normalize_json(self) -> Optional[Path]:
        """
        è§„èŒƒåŒ– JSON æ–‡ä»¶

        æŸ¥æ‰¾å¹¶é‡å‘½åä¸ºæ ‡å‡†åç§°ï¼šresult.json
        """
        # æŸ¥æ‰¾æ‰€æœ‰ .json æ–‡ä»¶ï¼ˆæ’é™¤å­ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼‰
        json_files = [
            f
            for f in self.output_dir.rglob("*.json")
            if not f.parent.name.startswith("page_")  # æ’é™¤ PaddleOCR-VL çš„åˆ†é¡µæ–‡ä»¶
        ]

        if not json_files:
            logger.info("â„¹ï¸  No JSON files found")
            return None

        # å¦‚æœå·²ç»æœ‰ result.jsonï¼Œç›´æ¥è¿”å›
        standard_json = self.output_dir / self.STANDARD_JSON_NAME
        if standard_json.exists():
            logger.info(f"âœ… Standard JSON file already exists: {standard_json.name}")
            return standard_json

        # é€‰æ‹©ä¸» JSON æ–‡ä»¶ï¼ˆä¼˜å…ˆé€‰æ‹© content_list.json æˆ–æœ€å¤§çš„æ–‡ä»¶ï¼‰
        main_json = None
        for f in json_files:
            if "content_list" in f.name or "result" in f.name:
                main_json = f
                break

        if not main_json:
            main_json = max(json_files, key=lambda f: f.stat().st_size)

        logger.info(f"ğŸ“„ Found main JSON: {main_json.relative_to(self.output_dir)}")

        # å¦‚æœä¸åœ¨æ ¹ç›®å½•ï¼Œç§»åŠ¨åˆ°æ ¹ç›®å½•
        if main_json.parent != self.output_dir:
            logger.info("   Moving to root directory...")
            shutil.copy2(main_json, standard_json)
        else:
            # é‡å‘½å
            logger.info(f"   Renaming to {self.STANDARD_JSON_NAME}...")
            main_json.rename(standard_json)

        return standard_json

    def _update_markdown_image_refs(self, markdown_file: Path):
        """
        æ›´æ–° Markdown æ–‡ä»¶ä¸­çš„å›¾ç‰‡å¼•ç”¨

        å°†æ‰€æœ‰å›¾ç‰‡è·¯å¾„ç»Ÿä¸€ä¸º images/xxx.jpg æ ¼å¼
        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. Markdown è¯­æ³•ï¼š![alt](path)
        2. HTML æ ‡ç­¾ï¼š<img src="path" ...>
        """
        try:
            content = markdown_file.read_text(encoding="utf-8")

            # 1. åŒ¹é… Markdown å›¾ç‰‡è¯­æ³•ï¼š![alt](path)
            md_img_pattern = r"!\[([^\]]*)\]\(([^)]+)\)"

            def replace_md_path(match):
                alt_text = match.group(1)
                img_path = match.group(2)

                # æå–æ–‡ä»¶å
                img_filename = Path(img_path).name

                # ç»Ÿä¸€ä¸º images/filename æ ¼å¼
                new_path = f"{self.STANDARD_IMAGE_DIR}/{img_filename}"

                return f"![{alt_text}]({new_path})"

            # 2. åŒ¹é… HTML img æ ‡ç­¾ï¼š<img src="path" ...>
            html_img_pattern = r'<img\s+([^>]*\s+)?src="([^"]+)"([^>]*)>'

            def replace_html_path(match):
                before_src = match.group(1) or ""
                img_path = match.group(2)
                after_src = match.group(3) or ""

                # æå–æ–‡ä»¶å
                img_filename = Path(img_path).name

                # ç»Ÿä¸€ä¸º images/filename æ ¼å¼
                new_path = f"{self.STANDARD_IMAGE_DIR}/{img_filename}"

                return f'<img {before_src}src="{new_path}"{after_src}>'

            # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
            new_content = re.sub(md_img_pattern, replace_md_path, content)
            new_content = re.sub(html_img_pattern, replace_html_path, new_content)

            # åªæœ‰å†…å®¹å˜åŒ–æ—¶æ‰å†™å…¥
            if new_content != content:
                markdown_file.write_text(new_content, encoding="utf-8")
                logger.info(f"âœ… Updated image references in {markdown_file.name}")
            else:
                logger.debug(f"â„¹ï¸  No image references to update in {markdown_file.name}")

        except Exception as e:
            logger.warning(f"âš ï¸  Failed to update image references: {e}")

    def _upload_images_to_rustfs(self, image_dir: Path) -> Dict[str, str]:
        """
        ä¸Šä¼ å›¾ç‰‡åˆ° RustFS å¯¹è±¡å­˜å‚¨

        Args:
            image_dir: å›¾ç‰‡ç›®å½•

        Returns:
            {æœ¬åœ°æ–‡ä»¶å: RustFS URL} çš„æ˜ å°„å­—å…¸
        """
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨ä¸éœ€è¦æ—¶åˆå§‹åŒ–
        try:
            from storage import RustFSClient

            if self._rustfs_client is None:
                self._rustfs_client = RustFSClient()

            # ç›´æ¥ä¸Šä¼ ï¼Œä½¿ç”¨æ—¥æœŸå‰ç¼€ (YYYYMMDD/çŸ­uuid.ext)
            logger.info(f"ğŸ“¤ Uploading images to RustFS: {image_dir}")
            url_mapping = self._rustfs_client.upload_directory(
                str(image_dir),
                prefix=None,  # ä¸ä½¿ç”¨é¢å¤–å‰ç¼€ï¼Œç›´æ¥ç”¨æ—¥æœŸåˆ†ç»„
            )

            return url_mapping

        except Exception as e:
            logger.error(f"âŒ Failed to initialize RustFS client: {e}")
            raise

    def _replace_markdown_urls(self, md_file: Path, url_mapping: Dict[str, str]):
        """
        æ›¿æ¢ Markdown ä¸­çš„å›¾ç‰‡è·¯å¾„ä¸º RustFS URL

        Args:
            md_file: Markdown æ–‡ä»¶
            url_mapping: {æœ¬åœ°æ–‡ä»¶å: RustFS URL} æ˜ å°„
        """
        try:
            content = md_file.read_text(encoding="utf-8")
            original_content = content
            replaced_count = 0

            logger.debug(f"ğŸ” Replacing URLs in {md_file.name}")
            logger.debug(f"   URL mapping: {url_mapping}")

            # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨ï¼ˆç»Ÿä¸€è½¬æ¢ä¸º HTML æ ¼å¼ï¼Œæ›´é€šç”¨ï¼‰
            for filename, url in url_mapping.items():
                # æ–¹å¼1: Markdown æ ¼å¼ -> HTML æ ¼å¼
                # ![alt](images/xxx.jpg) -> <img src="https://..." alt="alt">
                pattern1 = rf"!\[(.*?)\]\({self.STANDARD_IMAGE_DIR}/{re.escape(filename)}\)"
                matches1 = re.findall(pattern1, content)
                if matches1:
                    logger.debug(f"   Found Markdown pattern: {pattern1}")
                    logger.debug(f"   Matches: {matches1}")

                # è½¬æ¢ä¸º HTML æ ¼å¼ï¼ˆæ›´é€šç”¨ï¼Œå‰ç«¯æ¸²æŸ“å‹å¥½ï¼‰
                def markdown_to_html(match):
                    alt_text = match.group(1) or filename
                    return f'<img src="{url}" alt="{alt_text}">'

                new_content = re.sub(pattern1, markdown_to_html, content)
                if new_content != content:
                    replaced_count += 1
                    logger.debug(f"   âœ… Replaced Markdown -> HTML: {filename} -> {url}")
                content = new_content

                # æ–¹å¼2: HTML æ ¼å¼ -> æ›´æ–° URL
                # <img src="images/xxx.jpg"> -> <img src="https://...">
                pattern2 = rf'<img([^>]*?)src=["\']({self.STANDARD_IMAGE_DIR}/{re.escape(filename)})["\']([^>]*?)>'
                matches2 = re.findall(pattern2, content)
                if matches2:
                    logger.debug(f"   Found HTML pattern: {pattern2}")
                    logger.debug(f"   Matches: {matches2}")

                new_content = re.sub(pattern2, rf'<img\1src="{url}"\3>', content)
                if new_content != content:
                    replaced_count += 1
                    logger.debug(f"   âœ… Replaced HTML: {filename} -> {url}")
                content = new_content

            if content != original_content:
                md_file.write_text(content, encoding="utf-8")
                logger.info(f"âœ… Replaced {replaced_count} image URLs in {md_file.name}")
            else:
                logger.warning(f"âš ï¸  No replacements made in {md_file.name}")
                logger.debug(f"   Content preview (first 500 chars):\n{original_content[:500]}")

        except Exception as e:
            logger.error(f"âŒ Failed to replace URLs in Markdown: {e}")
            raise

    def _replace_json_urls(self, json_file: Path, url_mapping: Dict[str, str]):
        """
        æ›¿æ¢ JSON ä¸­çš„å›¾ç‰‡è·¯å¾„ä¸º RustFS URL

        Args:
            json_file: JSON æ–‡ä»¶
            url_mapping: {æœ¬åœ°æ–‡ä»¶å: RustFS URL} æ˜ å°„
        """
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            replaced_count = 0
            logger.debug(f"ğŸ” Replacing URLs in {json_file.name}")

            # é€’å½’æ›¿æ¢ JSON ä¸­çš„æ‰€æœ‰å›¾ç‰‡è·¯å¾„
            def replace_paths(obj, path=""):
                nonlocal replaced_count
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if isinstance(value, str):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡è·¯å¾„
                            for filename, url in url_mapping.items():
                                if filename in value and self.STANDARD_IMAGE_DIR in value:
                                    old_value = obj[key]
                                    obj[key] = url
                                    replaced_count += 1
                                    logger.debug(f"   âœ… Replaced JSON[{path}.{key}]: {old_value} -> {url}")
                                    break
                        else:
                            replace_paths(value, f"{path}.{key}")
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        replace_paths(item, f"{path}[{i}]")

            replace_paths(data)

            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            if replaced_count > 0:
                logger.info(f"âœ… Replaced {replaced_count} image URLs in {json_file.name}")
            else:
                logger.warning(f"âš ï¸  No replacements made in {json_file.name}")

        except Exception as e:
            logger.error(f"âŒ Failed to replace URLs in JSON: {e}")
            raise


def normalize_paddleocr_output(output_dir: Path) -> Dict[str, Any]:
    """
    ä¸“é—¨å¤„ç† PaddleOCR-VL çš„è¾“å‡ºè§„èŒƒåŒ–

    å¤„ç†æ­¥éª¤ï¼š
    1. ä¸ºæ¯é¡µçš„å›¾ç‰‡æ·»åŠ é¡µç å‰ç¼€ï¼ˆpage1_xxx.jpgï¼‰ï¼Œé¿å…å¤šé¡µå›¾ç‰‡åç§°å†²çª
    2. åˆå¹¶æ‰€æœ‰é¡µçš„ JSONï¼Œå¹¶ä¸º image å—æ·»åŠ  img_path å­—æ®µ
    3. æ›´æ–° Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨è·¯å¾„

    Args:
        output_dir: PaddleOCR-VL è¾“å‡ºç›®å½•ï¼ˆåŒ…å« page_N å­ç›®å½•ï¼‰

    Returns:
        è§„èŒƒåŒ–åçš„æ–‡ä»¶ä¿¡æ¯
    """
    output_dir = Path(output_dir)
    logger.info(f"ğŸ¤– Normalizing PaddleOCR-VL output: {output_dir}")

    STANDARD_IMAGE_DIR = "images"
    STANDARD_JSON_NAME = "result.json"

    # 1. å¤„ç†å›¾ç‰‡ï¼šé‡å‘½åå¹¶ç§»åŠ¨åˆ° images/ ç›®å½•
    standard_image_dir = output_dir / STANDARD_IMAGE_DIR
    standard_image_dir.mkdir(exist_ok=True)

    image_mapping = {}  # {page_idx: {åŸå§‹å: æ–°å}}
    image_counter = 1  # å…¨å±€ç´¯è¿›ç¼–å·

    page_dirs = sorted(output_dir.glob("page_*"))

    for page_dir in page_dirs:
        # æå–é¡µç ï¼ˆpage_1 -> 1ï¼‰
        try:
            page_num = int(page_dir.name.split("_")[1])
        except (IndexError, ValueError):
            logger.warning(f"âš ï¸  Invalid page directory: {page_dir.name}")
            continue

        # æŸ¥æ‰¾è¯¥é¡µçš„ imgs ç›®å½•
        imgs_dir = page_dir / "imgs"
        if not imgs_dir.exists():
            continue

        page_mapping = {}
        logger.info(f"ğŸ“ Processing {page_dir.name}/imgs/")

        # å¤„ç†è¯¥é¡µçš„æ‰€æœ‰å›¾ç‰‡
        for img_file in imgs_dir.iterdir():
            if not img_file.is_file():
                continue

            # ç”Ÿæˆæ–°æ–‡ä»¶åï¼šimage_001.jpg, image_002.jpg, ...
            file_ext = img_file.suffix
            new_name = f"image_{image_counter:03d}{file_ext}"
            new_path = standard_image_dir / new_name

            # ç§»åŠ¨å›¾ç‰‡
            shutil.move(str(img_file), str(new_path))
            page_mapping[img_file.name] = new_name
            image_counter += 1
            logger.debug(f"   {img_file.name} -> {new_name}")

        image_mapping[page_num - 1] = page_mapping  # page_idx ä» 0 å¼€å§‹

        # åˆ é™¤ç©ºçš„ imgs ç›®å½•
        try:
            imgs_dir.rmdir()
        except OSError:
            pass

    logger.info(f"âœ… Renamed {image_counter - 1} images with sequential numbering")

    # 2. å¤„ç† JSONï¼šåˆå¹¶æ‰€æœ‰é¡µå¹¶æ·»åŠ  img_path å­—æ®µ
    all_pages_data = []

    for page_dir in page_dirs:
        # æŸ¥æ‰¾è¯¥é¡µçš„ JSON æ–‡ä»¶ï¼ˆæ ¼å¼ï¼š*_res.jsonï¼‰
        json_files = list(page_dir.glob("*_res.json"))

        if not json_files:
            logger.warning(f"âš ï¸  No JSON file in {page_dir.name}")
            continue

        json_file = json_files[0]

        try:
            with open(json_file, "r", encoding="utf-8") as f:
                page_data = json.load(f)

            # è·å–é¡µç 
            page_idx = page_data.get("page_index", 0)

            # ä¸ºå›¾ç‰‡å—æ·»åŠ  img_path å­—æ®µ
            if "parsing_res_list" in page_data:
                page_img_mapping = image_mapping.get(page_idx, {})

                for block in page_data["parsing_res_list"]:
                    if block.get("block_label") == "image":
                        # æ ¹æ® bbox ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼ˆPaddleOCR çš„å‘½åè§„åˆ™ï¼‰
                        bbox = block.get("block_bbox", [])
                        if len(bbox) == 4:
                            # å›¾ç‰‡æ–‡ä»¶åæ ¼å¼ï¼šimg_in_image_box_{x1}_{y1}_{x2}_{y2}.jpg
                            img_name = f"img_in_image_box_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.jpg"

                            # æŸ¥æ‰¾å¯¹åº”çš„æ–°æ–‡ä»¶å
                            if img_name in page_img_mapping:
                                new_img_name = page_img_mapping[img_name]
                                # æ·»åŠ  img_path å­—æ®µï¼ˆå‚è€ƒ MinerU æ ¼å¼ï¼‰
                                block["img_path"] = f"{STANDARD_IMAGE_DIR}/{new_img_name}"
                                logger.debug(f"   Added img_path: {block['img_path']}")

            all_pages_data.append(page_data)
            logger.info(f"âœ… Processed {json_file.name}")

        except Exception as e:
            logger.warning(f"âš ï¸  Failed to process {json_file}: {e}")
            continue

    # ä¿å­˜åˆå¹¶åçš„ JSON
    if all_pages_data:
        standard_json = output_dir / STANDARD_JSON_NAME
        combined_data = {
            "pages": all_pages_data,
            "total_pages": len(all_pages_data),
            "format": "paddleocr-vl",
        }

        with open(standard_json, "w", encoding="utf-8") as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… Created {STANDARD_JSON_NAME} with {len(all_pages_data)} pages")

    # 3. å¤„ç† Markdownï¼šæ›´æ–°å›¾ç‰‡å¼•ç”¨
    # æŸ¥æ‰¾ result.md æ–‡ä»¶
    md_files = list(output_dir.rglob("*.md"))
    if md_files:
        # é€‰æ‹©æœ€å¤§çš„ .md æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ä¸»æ–‡ä»¶ï¼‰
        main_md = max(md_files, key=lambda f: f.stat().st_size)

        # å¦‚æœä¸åœ¨æ ¹ç›®å½•ï¼Œç§»åŠ¨åˆ°æ ¹ç›®å½•
        standard_md = output_dir / "result.md"
        if main_md != standard_md:
            if main_md.parent != output_dir:
                shutil.copy2(main_md, standard_md)
            else:
                main_md.rename(standard_md)
            main_md = standard_md

        # æ›´æ–° Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨
        try:
            content = main_md.read_text(encoding="utf-8")

            # æ„å»ºå®Œæ•´çš„å›¾ç‰‡æ˜ å°„ï¼ˆæ‰€æœ‰é¡µçš„å›¾ç‰‡ï¼‰
            full_image_mapping = {}
            for page_mapping in image_mapping.values():
                full_image_mapping.update(page_mapping)

            # æ›¿æ¢ Markdown å›¾ç‰‡è¯­æ³•ï¼š![alt](path)
            md_img_pattern = r"!\[([^\]]*)\]\(([^)]+)\)"

            def replace_md_path(match):
                alt_text = match.group(1)
                img_path = match.group(2)
                img_filename = Path(img_path).name

                # æŸ¥æ‰¾æ–°æ–‡ä»¶å
                if img_filename in full_image_mapping:
                    new_name = full_image_mapping[img_filename]
                    return f"![{alt_text}]({STANDARD_IMAGE_DIR}/{new_name})"
                return match.group(0)

            # æ›¿æ¢ HTML img æ ‡ç­¾ï¼š<img src="path" ...>
            html_img_pattern = r'<img\s+([^>]*\s+)?src="([^"]+)"([^>]*)>'

            def replace_html_path(match):
                before_src = match.group(1) or ""
                img_path = match.group(2)
                after_src = match.group(3) or ""
                img_filename = Path(img_path).name

                # æŸ¥æ‰¾æ–°æ–‡ä»¶å
                if img_filename in full_image_mapping:
                    new_name = full_image_mapping[img_filename]
                    return f'<img {before_src}src="{STANDARD_IMAGE_DIR}/{new_name}"{after_src}>'
                return match.group(0)

            # æ‰§è¡Œæ›¿æ¢
            new_content = re.sub(md_img_pattern, replace_md_path, content)
            new_content = re.sub(html_img_pattern, replace_html_path, new_content)

            if new_content != content:
                main_md.write_text(new_content, encoding="utf-8")
                logger.info(f"âœ… Updated image references in {main_md.name}")

        except Exception as e:
            logger.warning(f"âš ï¸  Failed to update markdown image references: {e}")

    logger.info("âœ… PaddleOCR-VL normalization complete")

    return {
        "markdown_file": standard_md if md_files else None,
        "json_file": output_dir / STANDARD_JSON_NAME if all_pages_data else None,
        "image_dir": standard_image_dir,
        "image_count": image_counter - 1,
    }


def normalize_output(output_dir: Path) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè§„èŒƒåŒ–è¾“å‡ºç›®å½•

    è‡ªåŠ¨æ£€æµ‹è¾“å‡ºç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„è§„èŒƒåŒ–æ–¹æ³•ï¼š
    - å¦‚æœæ£€æµ‹åˆ° page_N ç›®å½•ï¼Œä½¿ç”¨ PaddleOCR-VL ä¸“ç”¨è§„èŒƒåŒ–
    - å¦åˆ™ä½¿ç”¨é€šç”¨è§„èŒƒåŒ–

    Args:
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è§„èŒƒåŒ–åçš„æ–‡ä»¶ä¿¡æ¯
    """
    output_dir = Path(output_dir)

    # æ£€æµ‹æ˜¯å¦æ˜¯ PaddleOCR-VL è¾“å‡ºï¼ˆæœ‰ page_N å­ç›®å½•ï¼‰
    page_dirs = list(output_dir.glob("page_*"))

    if page_dirs:
        logger.info("ğŸ¤– Detected PaddleOCR-VL output format")
        return normalize_paddleocr_output(output_dir)
    else:
        # é€šç”¨è§„èŒƒåŒ–
        normalizer = OutputNormalizer(output_dir)
        return normalizer.normalize()
