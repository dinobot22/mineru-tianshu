"""
MinerU Tianshu - LitServe Worker
å¤©æ¢ LitServe Worker

ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å° - GPU Worker
æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç†
ä½¿ç”¨ LitServe å®ç° GPU èµ„æºçš„è‡ªåŠ¨è´Ÿè½½å‡è¡¡
Worker ä¸»åŠ¨å¾ªç¯æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
import argparse
import multiprocessing
import socket
import litserve as ls

from pathlib import Path
from typing import Optional
from loguru import logger

from utils.env_utils import load_env_if_not_loaded
from utils.perse_uitls import parse_list_arg
from utils.litserve_utils import (
    apply_litserve_patch,
    verify_pytorch_cuda,
    init_task_db,
    resolve_auto_accelerator,
    configure_model_source,
)
from utils.pdf_utils import get_pdf_page_count, split_pdf_file
from output_normalizer import normalize_output
from task_db import TaskDB

# base config
load_env_if_not_loaded()  # å¦‚æœ.envæœªåŠ è½½çš„è¯, åŠ è½½ç¯å¢ƒå˜é‡
apply_litserve_patch()  # åº”ç”¨ LitServe è¡¥ä¸, ä¿®å¤ MCP ä¸ mcp>=1.1.0 ä¸å…¼å®¹é—®é¢˜
# é…ç½®æ¨¡å‹ä¸‹è½½æºï¼ˆå¿…é¡»åœ¨ MinerU åˆå§‹åŒ–ä¹‹å‰ï¼‰:æ”¯æŒ: modelscope, huggingface, auto (é»˜è®¤)
configure_model_source(model_source=os.getenv("MODEL_DOWNLOAD_SOURCE", "auto").lower())

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ MinerU
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# å»¶è¿Ÿå¯¼å…¥ MinerUï¼Œé¿å…è¿‡æ—©åˆå§‹åŒ– CUDA
# MinerU ä¼šåœ¨ setup() è®¾ç½® CUDA_VISIBLE_DEVICES åå†å¯¼å…¥
# from mineru.cli.common import do_parse
# from mineru.utils.model_utils import get_vram, clean_memory

# å¯¼å…¥ importlib ç”¨äºæ£€æŸ¥æ¨¡å—å¯ç”¨æ€§
import importlib.util

# å°è¯•å¯¼å…¥ markitdown
try:
    from markitdown import MarkItDown

    MARKITDOWN_AVAILABLE = True
except ImportError:
    MARKITDOWN_AVAILABLE = False
    logger.warning("âš ï¸  markitdown not available, Office format parsing will be disabled")

# æ£€æŸ¥ PaddleOCR-VL æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_AVAILABLE = importlib.util.find_spec("paddleocr_vl") is not None
if PADDLEOCR_VL_AVAILABLE:
    logger.info("âœ… PaddleOCR-VL engine available")
else:
    logger.info("â„¹ï¸  PaddleOCR-VL not available (optional)")

# æ£€æŸ¥ PaddleOCR-VL-VLLM æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
PADDLEOCR_VL_VLLM_AVAILABLE = importlib.util.find_spec("paddleocr_vl_vllm") is not None
if PADDLEOCR_VL_VLLM_AVAILABLE:
    logger.info("âœ… PaddleOCR-VL-VLLM engine available")
else:
    logger.info("â„¹ï¸  PaddleOCR-VL-VLLM not available (optional)")

# æ£€æŸ¥ MinerU Pipeline æ˜¯å¦å¯ç”¨
MINERU_PIPELINE_AVAILABLE = importlib.util.find_spec("mineru_pipeline") is not None
if MINERU_PIPELINE_AVAILABLE:
    logger.info("âœ… MinerU Pipeline engine available")
else:
    logger.info("â„¹ï¸  MinerU Pipeline not available (optional)")

# å°è¯•å¯¼å…¥ SenseVoice éŸ³é¢‘å¤„ç†
SENSEVOICE_AVAILABLE = importlib.util.find_spec("audio_engines") is not None
if SENSEVOICE_AVAILABLE:
    logger.info("âœ… SenseVoice audio engine available")
else:
    logger.info("â„¹ï¸  SenseVoice not available (optional)")

# å°è¯•å¯¼å…¥è§†é¢‘å¤„ç†å¼•æ“
VIDEO_ENGINE_AVAILABLE = importlib.util.find_spec("video_engines") is not None
if VIDEO_ENGINE_AVAILABLE:
    logger.info("âœ… Video processing engine available")
else:
    logger.info("â„¹ï¸  Video processing engine not available (optional)")

# æ£€æŸ¥æ°´å°å»é™¤å¼•æ“æ˜¯å¦å¯ç”¨ï¼ˆä¸è¦å¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ– CUDAï¼‰
WATERMARK_REMOVAL_AVAILABLE = importlib.util.find_spec("remove_watermark") is not None
if WATERMARK_REMOVAL_AVAILABLE:
    logger.info("âœ… Watermark removal engine available")
else:
    logger.info("â„¹ï¸  Watermark removal engine not available (optional)")

# å°è¯•å¯¼å…¥æ ¼å¼å¼•æ“ï¼ˆä¸“ä¸šé¢†åŸŸæ ¼å¼æ”¯æŒï¼‰
try:
    from format_engines import FormatEngineRegistry, FASTAEngine, GenBankEngine

    # æ³¨å†Œæ‰€æœ‰å¼•æ“
    FormatEngineRegistry.register(FASTAEngine())
    FormatEngineRegistry.register(GenBankEngine())

    FORMAT_ENGINES_AVAILABLE = True
    logger.info("âœ… Format engines available")
    logger.info(f"   Supported extensions: {', '.join(FormatEngineRegistry.get_supported_extensions())}")
except ImportError as e:
    FORMAT_ENGINES_AVAILABLE = False
    logger.info(f"â„¹ï¸  Format engines not available (optional): {e}")


class MinerUWorkerAPI(ls.LitAPI):
    def __init__(
        self,
        output_dir=None,
        poll_interval=0.5,
        enable_worker_loop=True,
        paddleocr_vl_vllm_engine_enabled=False,
        paddleocr_vl_vllm_api_list=None,
    ):
        """
        åˆå§‹åŒ– APIï¼šç›´æ¥åœ¨è¿™é‡Œæ¥æ”¶æ‰€æœ‰éœ€è¦çš„å‚æ•°
        """
        super().__init__()
        self.output_dir = output_dir
        self.poll_interval = poll_interval
        self.enable_worker_loop = enable_worker_loop
        self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
        self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list or []
        ctx = multiprocessing.get_context("spawn")
        self._global_worker_counter = ctx.Value("i", 0)

    def setup(self, device):
        """
        åˆå§‹åŒ– Worker (æ¯ä¸ª GPU ä¸Šè°ƒç”¨ä¸€æ¬¡)

        Args:
            device: è®¾å¤‡ ID (cuda:0, cuda:1, cpu ç­‰)
        """
        ## é…ç½®æ¯ä¸ª Worker çš„å…¨å±€ç´¢å¼•å¹¶å°è¯•æ€§åˆ†é…self.paddleocr_vl_vllm_api
        with self._global_worker_counter.get_lock():
            my_global_index = self._global_worker_counter.value
            self._global_worker_counter.value += 1
        logger.info(f"ğŸ”¢ [Init] I am Global Worker #{my_global_index} (on {device})")
        if self.paddleocr_vl_vllm_engine_enabled and len(self.paddleocr_vl_vllm_api_list) > 0:
            assigned_api = self.paddleocr_vl_vllm_api_list[my_global_index % len(self.paddleocr_vl_vllm_api_list)]
            self.paddleocr_vl_vllm_api = assigned_api
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: {assigned_api}")
        else:
            self.paddleocr_vl_vllm_api = None
            logger.info(f"ğŸ”§ Worker #{my_global_index} assigned Paddle OCR VL API: None")

        # ============================================================================
        # ã€å…³é”®ã€‘ç¬¬ä¸€æ­¥ï¼šç«‹å³è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆå¿…é¡»åœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰ï¼‰
        # ============================================================================
        # LitServe ä¸ºæ¯ä¸ª worker è¿›ç¨‹åˆ†é…ä¸åŒçš„ device (cuda:0, cuda:1, ...)
        # æˆ‘ä»¬éœ€è¦åœ¨å¯¼å…¥ä»»ä½• CUDA åº“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå®ç°è¿›ç¨‹çº§ GPU éš”ç¦»
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            # ã€å…³é”®ã€‘è®¾ç½® MinerU çš„è®¾å¤‡æ¨¡å¼ä¸º cuda:0
            # å› ä¸ºè®¾ç½®äº† CUDA_VISIBLE_DEVICES åï¼Œè¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€å¼ å¡ï¼ˆé€»è¾‘ ID å˜ä¸º 0ï¼‰
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
            logger.info(f"ğŸ¯ [GPU Isolation] Set CUDA_VISIBLE_DEVICES={gpu_id} (Physical GPU {gpu_id} â†’ Logical GPU 0)")
            logger.info("ğŸ¯ [GPU Isolation] Set MINERU_DEVICE_MODE=cuda:0")

        self.device = device
        # ============================================================================
        # ç¬¬äºŒæ­¥ï¼šç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ MinerU äº†ï¼ˆCUDA_VISIBLE_DEVICES å·²è®¾ç½®ï¼‰
        # ============================================================================
        global get_vram, clean_memory
        from mineru.utils.model_utils import get_vram, clean_memory

        # é…ç½® MinerU çš„ VRAM è®¾ç½®
        if os.getenv("MINERU_VIRTUAL_VRAM_SIZE", None) is None:
            device_mode = os.environ.get("MINERU_DEVICE_MODE", str(device))
            if device_mode.startswith("cuda") or device_mode.startswith("npu"):
                try:
                    # æ³¨æ„ï¼šget_vram éœ€è¦ä¼ å…¥è®¾å¤‡å­—ç¬¦ä¸²ï¼ˆå¦‚ "cuda:0"ï¼‰
                    vram = round(get_vram(device_mode))
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = str(vram)
                    logger.info(f"ğŸ® [MinerU VRAM] Detected: {vram}GB")
                except Exception as e:
                    os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "8"  # é»˜è®¤å€¼
                    logger.warning(f"âš ï¸  Failed to detect VRAM, using default: 8GB ({e})")
            else:
                os.environ["MINERU_VIRTUAL_VRAM_SIZE"] = "1"
                logger.info("ğŸ® [MinerU VRAM] CPU mode, set to 1GB")

        verify_pytorch_cuda()  # éªŒè¯ PyTorch CUDA è®¾ç½®

        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä»»åŠ¡æ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå…¼å®¹ Docker å’Œæœ¬åœ°ï¼‰
        db_path = os.getenv("DATABASE_PATH", "./app_data/mineru_tianshu.db")
        self.task_db = init_task_db(TaskDB, db_path)

        # Worker çŠ¶æ€
        self.running = True
        self.current_task_id = None

        # ç”Ÿæˆå”¯ä¸€çš„ worker_id: tianshu-{hostname}-{device}-{pid}
        hostname = socket.gethostname()
        pid = os.getpid()
        self.worker_id = f"tianshu-{hostname}-{device}-{pid}"
        # å­è¿›ç¨‹ï¼ˆsetup ä¸­ï¼‰ï¼š

        # åˆå§‹åŒ–å¯é€‰çš„å¤„ç†å¼•æ“
        self.markitdown = MarkItDown() if MARKITDOWN_AVAILABLE else None
        self.mineru_pipeline_engine = None  # å»¶è¿ŸåŠ è½½
        self.paddleocr_vl_engine = None  # å»¶è¿ŸåŠ è½½
        self.paddleocr_vl_vllm_engine = None  # å»¶è¿ŸåŠ è½½
        self.sensevoice_engine = None  # å»¶è¿ŸåŠ è½½
        self.video_engine = None  # å»¶è¿ŸåŠ è½½
        self.watermark_handler = None  # å»¶è¿ŸåŠ è½½

        logger.success(f"\n\n{'=' * 20} ğŸš€ Initializing New Worker Instance: {self.worker_id} {'=' * 40}\n")
        logger.info(f"ğŸ“ Device: {device}")
        logger.info(f"ğŸ“‚ Output Dir: {self.output_dir}")
        logger.info(f"ğŸ—ƒï¸  Database: {db_path}")
        logger.info(f"ğŸ”„ Worker Loop: {'Enabled' if self.enable_worker_loop else 'Disabled'}")
        if self.enable_worker_loop:
            logger.info(f"â±ï¸  Poll Interval: {self.poll_interval}s")
        logger.info("")

        # æ‰“å°å¯ç”¨çš„å¼•æ“
        logger.info("ğŸ“¦ Available Engines:")
        logger.info(f"   â€¢ MarkItDown: {'âœ…' if MARKITDOWN_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ MinerU Pipeline: {'âœ…' if MINERU_PIPELINE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ PaddleOCR-VL: {'âœ…' if PADDLEOCR_VL_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ SenseVoice: {'âœ…' if SENSEVOICE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Video Engine: {'âœ…' if VIDEO_ENGINE_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Watermark Removal: {'âœ…' if WATERMARK_REMOVAL_AVAILABLE else 'âŒ'}")
        logger.info(f"   â€¢ Format Engines: {'âœ…' if FORMAT_ENGINES_AVAILABLE else 'âŒ'}")
        logger.info("")

        # æ£€æµ‹å’Œåˆå§‹åŒ–æ°´å°å»é™¤å¼•æ“ï¼ˆä»… CUDAï¼‰
        if WATERMARK_REMOVAL_AVAILABLE and "cuda" in str(device).lower():
            try:
                logger.info("ğŸ¨ Initializing watermark removal engine...")
                # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿åœ¨ CUDA_VISIBLE_DEVICES è®¾ç½®ä¹‹å
                from remove_watermark.pdf_watermark_handler import PDFWatermarkHandler

                # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
                # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
                self.watermark_handler = PDFWatermarkHandler(device="cuda:0", use_lama=True)
                gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
                logger.info(f"âœ… Watermark removal engine initialized on cuda:0 (physical GPU {gpu_id})")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize watermark removal engine: {e}")
                self.watermark_handler = None

        logger.info("âœ… Worker ready")
        logger.info(f"   LitServe Device: {device}")
        logger.info(f"   MinerU Device Mode: {os.environ.get('MINERU_DEVICE_MODE', 'auto')}")
        logger.info(f"   MinerU VRAM: {os.environ.get('MINERU_VIRTUAL_VRAM_SIZE', 'unknown')}GB")
        if "cuda" in str(device).lower():
            physical_gpu = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"   Physical GPU: {physical_gpu}")

        # å¦‚æœå¯ç”¨äº† worker å¾ªç¯ï¼Œå¯åŠ¨åå°çº¿ç¨‹æ‹‰å–ä»»åŠ¡
        if self.enable_worker_loop:
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logger.info(f"ğŸ”„ Worker loop started (poll_interval={self.poll_interval}s)")
        else:
            logger.info("â¸ï¸  Worker loop disabled, waiting for manual triggers")

    def _worker_loop(self):
        """
        Worker åå°å¾ªç¯ï¼šæŒç»­æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†

        è¿™ä¸ªå¾ªç¯åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸æ–­æ£€æŸ¥æ˜¯å¦æœ‰æ–°ä»»åŠ¡
        ä¸€æ—¦æœ‰ä»»åŠ¡ï¼Œç«‹å³å¤„ç†ï¼Œå¤„ç†å®Œæˆåç»§ç»­å¾ªç¯
        """
        logger.info(f"ğŸ” {self.worker_id} started task polling loop")

        # è®°å½•åˆå§‹è¯Šæ–­ä¿¡æ¯
        try:
            stats = self.task_db.get_queue_stats()
            logger.info(f"ğŸ“Š Initial queue stats: {stats}")
            logger.info(f"ğŸ—ƒï¸  Database path: {self.task_db.db_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to get initial queue stats: {e}")

        loop_count = 0
        last_stats_log = 0
        stats_log_interval = 20  # æ¯20æ¬¡å¾ªç¯è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¦10ç§’ï¼‰

        while self.running:
            try:
                loop_count += 1

                # æ‹‰å–ä»»åŠ¡ï¼ˆåŸå­æ“ä½œï¼Œé˜²æ­¢é‡å¤å¤„ç†ï¼‰
                task = self.task_db.get_next_task(worker_id=self.worker_id)

                if task:
                    task_id = task["task_id"]
                    self.current_task_id = task_id
                    logger.info(
                        f"ğŸ“¥ {self.worker_id} pulled task: {task_id} (file: {task.get('file_name', 'unknown')})"
                    )

                    try:
                        # å¤„ç†ä»»åŠ¡
                        self._process_task(task)
                        logger.info(f"âœ… {self.worker_id} completed task: {task_id}")
                    except Exception as e:
                        logger.error(f"âŒ {self.worker_id} failed task {task_id}: {e}")
                        logger.exception(e)
                    finally:
                        self.current_task_id = None
                else:
                    # æ²¡æœ‰ä»»åŠ¡ï¼Œç©ºé—²ç­‰å¾…
                    # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ä»¥ä¾¿è¯Šæ–­
                    if loop_count - last_stats_log >= stats_log_interval:
                        try:
                            stats = self.task_db.get_queue_stats()
                            pending = stats.get("pending", 0)
                            processing = stats.get("processing", 0)

                            if pending > 0:
                                logger.warning(
                                    f"âš ï¸  {self.worker_id} polling (loop #{loop_count}): "
                                    f"{pending} pending tasks found but not pulled! "
                                    f"Processing: {processing}, Completed: {stats.get('completed', 0)}, "
                                    f"Failed: {stats.get('failed', 0)}"
                                )
                            elif loop_count % 100 == 0:  # æ¯50ç§’ï¼ˆ100æ¬¡å¾ªç¯ï¼‰è¾“å‡ºä¸€æ¬¡
                                logger.info(
                                    f"ğŸ’¤ {self.worker_id} idle (loop #{loop_count}): No pending tasks. Queue stats: {stats}"
                                )
                        except Exception as e:
                            logger.error(f"âŒ Failed to get queue stats: {e}")

                        last_stats_log = loop_count

                    time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"âŒ Worker loop error (loop #{loop_count}): {e}")
                logger.exception(e)
                time.sleep(self.poll_interval)

    def _process_task(self, task: dict):
        """
        å¤„ç†å•ä¸ªä»»åŠ¡

        Args:
            task: ä»»åŠ¡å­—å…¸ï¼ˆä»æ•°æ®åº“æ‹‰å–ï¼‰
        """
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}"))
        parent_task_id = task.get("parent_task_id")

        try:
            # æ ¹æ® backend é€‰æ‹©å¤„ç†æ–¹å¼ï¼ˆä» task å­—æ®µè¯»å–ï¼Œä¸æ˜¯ä» options è¯»å–ï¼‰
            backend = task.get("backend", "auto")

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            file_ext = Path(file_path).suffix.lower()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ† PDFï¼ˆä»…å¯¹éå­ä»»åŠ¡çš„ PDF è¿›è¡Œåˆ¤æ–­ï¼‰
            if file_ext == ".pdf" and not parent_task_id:
                if self._should_split_pdf(task_id, file_path, task, options):
                    # PDF å·²è¢«æ‹†åˆ†ï¼Œå½“å‰ä»»åŠ¡å·²è½¬ä¸ºçˆ¶ä»»åŠ¡ï¼Œç›´æ¥è¿”å›
                    return

            # 0. å¯é€‰ï¼šé¢„å¤„ç† - å»é™¤æ°´å°ï¼ˆä»… PDFï¼Œä½œä¸ºé¢„å¤„ç†æ­¥éª¤ï¼‰
            if file_ext == ".pdf" and options.get("remove_watermark", False) and self.watermark_handler:
                logger.info(f"ğŸ¨ [Preprocessing] Removing watermark from PDF: {file_path}")
                try:
                    cleaned_pdf_path = self._preprocess_remove_watermark(file_path, options)
                    file_path = str(cleaned_pdf_path)  # ä½¿ç”¨å»æ°´å°åçš„æ–‡ä»¶ç»§ç»­å¤„ç†
                    logger.info(f"âœ… [Preprocessing] Watermark removed, continuing with: {file_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ [Preprocessing] Watermark removal failed: {e}, continuing with original file")
                    # ç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶å¤„ç†

            # ç»Ÿä¸€çš„å¼•æ“è·¯ç”±é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ backendï¼Œå¦åˆ™è‡ªåŠ¨é€‰æ‹©
            result = None  # åˆå§‹åŒ– result

            # 1. ç”¨æˆ·æŒ‡å®šäº†éŸ³é¢‘å¼•æ“
            if backend == "sensevoice":
                if not SENSEVOICE_AVAILABLE:
                    raise ValueError("SenseVoice engine is not available")
                logger.info(f"ğŸ¤ Processing with SenseVoice: {file_path}")
                result = self._process_audio(file_path, options)

            # 3. ç”¨æˆ·æŒ‡å®šäº†è§†é¢‘å¼•æ“
            elif backend == "video":
                if not VIDEO_ENGINE_AVAILABLE:
                    raise ValueError("Video processing engine is not available")
                logger.info(f"ğŸ¬ Processing with video engine: {file_path}")
                result = self._process_video(file_path, options)

            # 4. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL
            elif backend == "paddleocr-vl":
                if not PADDLEOCR_VL_AVAILABLE:
                    raise ValueError("PaddleOCR-VL engine is not available")
                logger.info(f"ğŸ” Processing with PaddleOCR-VL: {file_path}")
                result = self._process_with_paddleocr_vl(file_path, options)

            # 5. ç”¨æˆ·æŒ‡å®šäº† PaddleOCR-VL-VLLM
            elif backend == "paddleocr-vl-vllm":
                if (
                    not PADDLEOCR_VL_VLLM_AVAILABLE
                    or not self.paddleocr_vl_vllm_engine_enabled
                    or len(self.paddleocr_vl_vllm_api_list) == 0
                ):
                    raise ValueError("PaddleOCR-VL-VLLM engine is not available")
                logger.info(f"ğŸ” Processing with PaddleOCR-VL-VLLM: {file_path}")
                result = self._process_with_paddleocr_vl_vllm(file_path, options)
            # 6. ç”¨æˆ·æŒ‡å®šäº† MinerU Pipeline
            elif backend == "pipeline":
                if not MINERU_PIPELINE_AVAILABLE:
                    raise ValueError("MinerU Pipeline engine is not available")
                logger.info(f"ğŸ”§ Processing with MinerU Pipeline: {file_path}")
                result = self._process_with_mineru(file_path, options)

            # 6. ç”¨æˆ·æŒ‡å®šäº† FASTA æˆ– GenBank æ ¼å¼å¼•æ“
            elif backend in ["fasta", "genbank"]:
                if FORMAT_ENGINES_AVAILABLE:
                    engine = FormatEngineRegistry.get_engine(backend)
                    if engine is not None:
                        logger.info(f"ğŸ§¬ Processing with format engine: {backend}")
                        result = self._process_with_format_engine(file_path, options, engine_name=backend)
                    else:
                        raise ValueError(
                            f"Unknown backend: {backend}. "
                            f"Supported backends: auto, pipeline, paddleocr-vl, sensevoice, video, fasta, genbank"
                        )
                else:
                    # æ ¼å¼å¼•æ“ä¸å¯ç”¨
                    raise ValueError(
                        f"Unknown backend: {backend}. "
                        f"Supported backends: auto, pipeline, paddleocr-vl, sensevoice, video"
                    )

            # auto æ¨¡å¼ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©å¼•æ“
            elif backend == "auto":
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸“ä¸šæ ¼å¼ï¼ˆFASTA, GenBank ç­‰ï¼‰
                if FORMAT_ENGINES_AVAILABLE and FormatEngineRegistry.is_supported(file_path):
                    logger.info(f"ğŸ§¬ [Auto] Processing with format engine: {file_path}")
                    result = self._process_with_format_engine(file_path, options)

                # æ£€æŸ¥æ˜¯å¦æ˜¯éŸ³é¢‘æ–‡ä»¶
                elif file_ext in [".wav", ".mp3", ".flac", ".m4a", ".ogg"] and SENSEVOICE_AVAILABLE:
                    logger.info(f"ğŸ¤ [Auto] Processing audio file: {file_path}")
                    result = self._process_audio(file_path, options)

                # æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶
                elif file_ext in [".mp4", ".avi", ".mkv", ".mov", ".flv", ".wmv"] and VIDEO_ENGINE_AVAILABLE:
                    logger.info(f"ğŸ¬ [Auto] Processing video file: {file_path}")
                    result = self._process_video(file_path, options)

                # é»˜è®¤ä½¿ç”¨ MinerU Pipeline å¤„ç† PDF/å›¾ç‰‡
                elif file_ext in [".pdf", ".png", ".jpg", ".jpeg"] and MINERU_PIPELINE_AVAILABLE:
                    logger.info(f"ğŸ”§ [Auto] Processing with MinerU Pipeline: {file_path}")
                    result = self._process_with_mineru(file_path, options)

                # å…œåº•ï¼šOffice æ–‡æ¡£/æ–‡æœ¬/HTML ä½¿ç”¨ MarkItDownï¼ˆå¦‚æœå¯ç”¨ï¼‰
                elif (
                    file_ext in [".docx", ".xlsx", ".pptx", ".doc", ".xls", ".ppt", ".html", ".txt", ".csv"]
                    and self.markitdown
                ):
                    logger.info(f"ğŸ“„ [Auto] Processing Office/Text file with MarkItDown: {file_path}")
                    result = self._process_with_markitdown(file_path)

                # æ²¡æœ‰åˆé€‚çš„å¤„ç†å™¨
                else:
                    supported_formats = "PDF, PNG, JPG (MinerU/PaddleOCR), Audio (SenseVoice), Video, FASTA, GenBank"
                    if self.markitdown:
                        supported_formats += ", Office/Text (MarkItDown)"
                    raise ValueError(
                        f"Unsupported file type: file={file_path}, ext={file_ext}. "
                        f"Supported formats: {supported_formats}"
                    )

            # æ£€æŸ¥ result æ˜¯å¦è¢«æ­£ç¡®èµ‹å€¼
            if result is None:
                raise ValueError(f"No result generated for backend: {backend}, file: {file_path}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            self.task_db.update_task_status(
                task_id=task_id,
                status="completed",
                result_path=result["result_path"],
                error_message=None,
            )

            # å¦‚æœæ˜¯å­ä»»åŠ¡,æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘åˆå¹¶
            if parent_task_id:
                parent_id_to_merge = self.task_db.on_child_task_completed(task_id)

                if parent_id_to_merge:
                    # æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ,æ‰§è¡Œåˆå¹¶
                    logger.info(f"ğŸ”€ All subtasks completed, merging results for parent task {parent_id_to_merge}")
                    try:
                        self._merge_parent_task_results(parent_id_to_merge)
                    except Exception as merge_error:
                        logger.error(f"âŒ Failed to merge parent task {parent_id_to_merge}: {merge_error}")
                        # æ ‡è®°çˆ¶ä»»åŠ¡ä¸ºå¤±è´¥
                        self.task_db.update_task_status(
                            parent_id_to_merge, "failed", error_message=f"Merge failed: {merge_error}"
                        )

            # æ¸…ç†æ˜¾å­˜ï¼ˆå¦‚æœæ˜¯ GPUï¼‰
            if "cuda" in str(self.device).lower():
                clean_memory()

        except Exception as e:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            error_msg = f"{type(e).__name__}: {str(e)}"
            self.task_db.update_task_status(task_id=task_id, status="failed", result_path=None, error_message=error_msg)

            # å¦‚æœæ˜¯å­ä»»åŠ¡å¤±è´¥,æ ‡è®°çˆ¶ä»»åŠ¡å¤±è´¥
            if parent_task_id:
                self.task_db.on_child_task_failed(task_id, error_msg)

            raise

    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        """
        ä½¿ç”¨ MinerU å¤„ç†æ–‡æ¡£

        æ³¨æ„ï¼š
        - MinerU çš„ do_parse åªæ¥å— PDF æ ¼å¼ï¼Œå›¾ç‰‡éœ€è¦å…ˆè½¬æ¢ä¸º PDF
        - CUDA_VISIBLE_DEVICES å·²åœ¨ setup() é˜¶æ®µè®¾ç½®ï¼ŒMinerU ä¼šè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„ GPU
        """
        # å»¶è¿ŸåŠ è½½ MinerU Pipelineï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if self.mineru_pipeline_engine is None:
            from mineru_pipeline import MinerUPipelineEngine

            # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
            # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
            self.mineru_pipeline_engine = MinerUPipelineEngine(device="cuda:0")
            gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"âœ… MinerU Pipeline engine loaded on cuda:0 (physical GPU {gpu_id})")

        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡ä»¶
        result = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        # æ³¨æ„ï¼šresult["result_path"] æ˜¯å®é™…åŒ…å« md æ–‡ä»¶çš„ç›®å½•ï¼ˆä¾‹å¦‚ {output_dir}/{file_name}/auto/ï¼‰
        # æˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸ªresult["result_path"] ä¸Šè¿è¡Œ normalize_output
        actual_output_dir = Path(result["result_path"])
        normalize_output(actual_output_dir)

        # MinerU Pipeline è¿”å›ç»“æ„ï¼š
        return {
            "result_path": result["result_path"],
            "content": result["markdown"],
            "json_path": result.get("json_path"),
            "json_content": result.get("json_content"),
        }

    def _process_with_markitdown(self, file_path: str) -> dict:
        """ä½¿ç”¨ MarkItDown å¤„ç† Office æ–‡æ¡£"""
        if not self.markitdown:
            raise RuntimeError("MarkItDown is not available")

        # å¤„ç†æ–‡ä»¶
        result = self.markitdown.convert(file_path)

        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç»“æœåˆ°ç›®å½•ä¸­
        output_file = output_dir / f"{Path(file_path).stem}_markitdown.md"
        output_file.write_text(result.text_content, encoding="utf-8")

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        normalize_output(output_dir)

        # è¿”å›ç›®å½•è·¯å¾„ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
        return {"result_path": str(output_dir), "content": result.text_content}

    def _process_with_paddleocr_vl(self, file_path: str, options: dict) -> dict:
        """ä½¿ç”¨ PaddleOCR-VL å¤„ç†å›¾ç‰‡æˆ– PDF"""
        # å»¶è¿ŸåŠ è½½ PaddleOCR-VLï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if self.paddleocr_vl_engine is None:
            from paddleocr_vl import PaddleOCRVLEngine

            # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
            # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
            self.paddleocr_vl_engine = PaddleOCRVLEngine(device="cuda:0")
            gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"âœ… PaddleOCR-VL engine loaded on cuda:0 (physical GPU {gpu_id})")

        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡ä»¶ï¼ˆparse æ–¹æ³•éœ€è¦ output_pathï¼‰
        result = self.paddleocr_vl_engine.parse(file_path, output_path=str(output_dir))

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        normalize_output(output_dir)

        # è¿”å›ç»“æœ
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_with_paddleocr_vl_vllm(self, file_path: str, options: dict) -> dict:
        """ä½¿ç”¨ PaddleOCR-VL VLLM å¤„ç†å›¾ç‰‡æˆ– PDF"""
        # å»¶è¿ŸåŠ è½½ PaddleOCR-VLï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if self.paddleocr_vl_vllm_engine is None:
            from paddleocr_vl_vllm import PaddleOCRVLVLLMEngine

            # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
            # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
            self.paddleocr_vl_vllm_engine = PaddleOCRVLVLLMEngine(
                device="cuda:0", vllm_api_base=self.paddleocr_vl_vllm_api
            )
            gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"âœ… PaddleOCR-VL engine loaded on cuda:0 (physical GPU {gpu_id})")

        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡ä»¶ï¼ˆparse æ–¹æ³•éœ€è¦ output_pathï¼‰
        result = self.paddleocr_vl_vllm_engine.parse(file_path, output_path=str(output_dir))

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        normalize_output(output_dir, handle_method="paddleocr-vl")

        # è¿”å›ç»“æœ
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_audio(self, file_path: str, options: dict) -> dict:
        """ä½¿ç”¨ SenseVoice å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
        # å»¶è¿ŸåŠ è½½ SenseVoiceï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if self.sensevoice_engine is None:
            from audio_engines import SenseVoiceEngine

            # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
            # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
            self.sensevoice_engine = SenseVoiceEngine(device="cuda:0")
            gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"âœ… SenseVoice engine loaded on cuda:0 (physical GPU {gpu_id})")

        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†éŸ³é¢‘ï¼ˆparse æ–¹æ³•éœ€è¦ output_path å‚æ•°ï¼‰
        result = self.sensevoice_engine.parse(
            audio_path=file_path,
            output_path=str(output_dir),
            language=options.get("lang", "auto"),
            use_itn=options.get("use_itn", True),
            enable_speaker_diarization=options.get("enable_speaker_diarization", False),  # ä» API å‚æ•°æ§åˆ¶
        )

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        normalize_output(output_dir)

        # SenseVoice è¿”å›ç»“æ„ï¼š
        # {
        #   "success": True,
        #   "output_path": str,
        #   "markdown": str,
        #   "markdown_file": str,
        #   "json_file": str,
        #   "json_data": dict,
        #   "result": dict
        # }
        return {"result_path": str(output_dir), "content": result.get("markdown", "")}

    def _process_video(self, file_path: str, options: dict) -> dict:
        """ä½¿ç”¨è§†é¢‘å¤„ç†å¼•æ“å¤„ç†è§†é¢‘æ–‡ä»¶"""
        # å»¶è¿ŸåŠ è½½è§†é¢‘å¼•æ“ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if self.video_engine is None:
            from video_engines import VideoProcessingEngine

            # æ³¨æ„ï¼šç”±äºåœ¨ setup() ä¸­å·²è®¾ç½® CUDA_VISIBLE_DEVICESï¼Œ
            # è¯¥è¿›ç¨‹åªèƒ½çœ‹åˆ°ä¸€ä¸ª GPUï¼ˆæ˜ å°„ä¸º cuda:0ï¼‰
            self.video_engine = VideoProcessingEngine(device="cuda:0")
            gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "?")
            logger.info(f"âœ… Video processing engine loaded on cuda:0 (physical GPU {gpu_id})")

        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†è§†é¢‘
        result = self.video_engine.parse(
            video_path=file_path,
            output_path=str(output_dir),
            language=options.get("lang", "auto"),
            use_itn=options.get("use_itn", True),
            keep_audio=options.get("keep_audio", False),
            enable_keyframe_ocr=options.get("enable_keyframe_ocr", False),
            ocr_backend=options.get("ocr_backend", "paddleocr-vl"),
            keep_keyframes=options.get("keep_keyframes", False),
        )

        # ä¿å­˜ç»“æœï¼ˆMarkdown æ ¼å¼ï¼‰
        output_file = output_dir / f"{Path(file_path).stem}_video_analysis.md"
        output_file.write_text(result["markdown"], encoding="utf-8")

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        normalize_output(output_dir)

        return {"result_path": str(output_dir), "content": result["markdown"]}

    def _preprocess_remove_watermark(self, file_path: str, options: dict) -> Path:
        """
        é¢„å¤„ç†ï¼šå»é™¤ PDF æ°´å°

        è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå»é™¤æ°´å°åçš„æ–‡ä»¶ä¼šè¢«åç»­çš„è§£æå¼•æ“å¤„ç†

        è¿”å›ï¼š
            å»é™¤æ°´å°åçš„ PDF è·¯å¾„

        æ”¯æŒçš„ options å‚æ•°ï¼š
            - auto_detect: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹ PDF ç±»å‹ï¼ˆé»˜è®¤ Trueï¼‰
            - force_scanned: å¼ºåˆ¶ä½¿ç”¨æ‰«æä»¶æ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼‰
            - remove_text: æ˜¯å¦åˆ é™¤æ–‡æœ¬å¯¹è±¡ï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
            - remove_images: æ˜¯å¦åˆ é™¤å›¾ç‰‡å¯¹è±¡ï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
            - remove_annotations: æ˜¯å¦åˆ é™¤æ³¨é‡Šï¼ˆå¯ç¼–è¾‘ PDFï¼Œé»˜è®¤ Trueï¼‰
            - keywords: æ–‡æœ¬å…³é”®è¯åˆ—è¡¨ï¼ˆå¯ç¼–è¾‘ PDFï¼Œåªåˆ é™¤åŒ…å«è¿™äº›å…³é”®è¯çš„æ–‡æœ¬ï¼‰
            - dpi: è½¬æ¢åˆ†è¾¨ç‡ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 200ï¼‰
            - conf_threshold: YOLO ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 0.35ï¼‰
            - dilation: æ©ç è†¨èƒ€ï¼ˆæ‰«æä»¶ PDFï¼Œé»˜è®¤ 10ï¼‰
        """
        if not self.watermark_handler:
            raise RuntimeError("Watermark removal is not available (CUDA required)")

        # è®¾ç½®è¾“å‡ºè·¯å¾„
        output_file = Path(self.output_dir) / f"{Path(file_path).stem}_no_watermark.pdf"

        # æ„å»ºå‚æ•°å­—å…¸ï¼ˆåªä¼ é€’å®é™…æä¾›çš„å‚æ•°ï¼‰
        kwargs = {}

        # é€šç”¨å‚æ•°
        if "auto_detect" in options:
            kwargs["auto_detect"] = options["auto_detect"]
        if "force_scanned" in options:
            kwargs["force_scanned"] = options["force_scanned"]

        # å¯ç¼–è¾‘ PDF å‚æ•°
        if "remove_text" in options:
            kwargs["remove_text"] = options["remove_text"]
        if "remove_images" in options:
            kwargs["remove_images"] = options["remove_images"]
        if "remove_annotations" in options:
            kwargs["remove_annotations"] = options["remove_annotations"]
        if "watermark_keywords" in options:
            kwargs["keywords"] = options["watermark_keywords"]

        # æ‰«æä»¶ PDF å‚æ•°
        if "watermark_dpi" in options:
            kwargs["dpi"] = options["watermark_dpi"]
        if "watermark_conf_threshold" in options:
            kwargs["conf_threshold"] = options["watermark_conf_threshold"]
        if "watermark_dilation" in options:
            kwargs["dilation"] = options["watermark_dilation"]

        # å»é™¤æ°´å°ï¼ˆè¿”å›è¾“å‡ºè·¯å¾„ï¼‰
        cleaned_pdf_path = self.watermark_handler.remove_watermark(
            input_path=file_path, output_path=str(output_file), **kwargs
        )

        return cleaned_pdf_path

    def _should_split_pdf(self, task_id: str, file_path: str, task: dict, options: dict) -> bool:
        """
        åˆ¤æ–­ PDF æ˜¯å¦éœ€è¦æ‹†åˆ†ï¼Œå¦‚æœéœ€è¦åˆ™æ‰§è¡Œæ‹†åˆ†

        Args:
            task_id: ä»»åŠ¡ID
            file_path: PDF æ–‡ä»¶è·¯å¾„
            task: ä»»åŠ¡å­—å…¸
            options: å¤„ç†é€‰é¡¹

        Returns:
            bool: True è¡¨ç¤ºå·²æ‹†åˆ†ï¼ŒFalse è¡¨ç¤ºä¸éœ€è¦æ‹†åˆ†
        """

        # è¯»å–é…ç½®
        pdf_split_enabled = os.getenv("PDF_SPLIT_ENABLED", "true").lower() == "true"
        if not pdf_split_enabled:
            return False

        pdf_split_threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "500"))
        pdf_split_chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "500"))

        try:
            # å¿«é€Ÿè¯»å– PDF é¡µæ•°ï¼ˆåªè¯»å…ƒæ•°æ®ï¼‰
            page_count = get_pdf_page_count(Path(file_path))
            logger.info(f"ğŸ“„ PDF has {page_count} pages (threshold: {pdf_split_threshold})")

            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‹†åˆ†
            if page_count <= pdf_split_threshold:
                return False

            logger.info(
                f"ğŸ”€ Large PDF detected ({page_count} pages), splitting into chunks of {pdf_split_chunk_size} pages"
            )

            # å°†å½“å‰ä»»åŠ¡è½¬ä¸ºçˆ¶ä»»åŠ¡
            self.task_db.convert_to_parent_task(task_id, child_count=0)

            # æ‹†åˆ† PDF æ–‡ä»¶
            split_dir = Path(self.output_dir) / "splits" / task_id
            split_dir.mkdir(parents=True, exist_ok=True)

            chunks = split_pdf_file(
                pdf_path=Path(file_path),
                output_dir=split_dir,
                chunk_size=pdf_split_chunk_size,
                parent_task_id=task_id,
            )

            logger.info(f"âœ‚ï¸  PDF split into {len(chunks)} chunks")

            # ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºå­ä»»åŠ¡
            backend = task.get("backend", "auto")
            priority = task.get("priority", 0)
            user_id = task.get("user_id")

            for chunk_info in chunks:
                # å¤åˆ¶é€‰é¡¹å¹¶æ·»åŠ åˆ†å—ä¿¡æ¯
                chunk_options = options.copy()
                chunk_options["chunk_info"] = {
                    "start_page": chunk_info["start_page"],
                    "end_page": chunk_info["end_page"],
                    "page_count": chunk_info["page_count"],
                }

                # åˆ›å»ºå­ä»»åŠ¡
                child_task_id = self.task_db.create_child_task(
                    parent_task_id=task_id,
                    file_name=f"{Path(file_path).stem}_pages_{chunk_info['start_page']}-{chunk_info['end_page']}.pdf",
                    file_path=chunk_info["path"],
                    backend=backend,
                    options=chunk_options,
                    priority=priority,
                    user_id=user_id,
                )

                logger.info(
                    f"  âœ… Created subtask {child_task_id}: pages {chunk_info['start_page']}-{chunk_info['end_page']}"
                )

            # æ›´æ–°çˆ¶ä»»åŠ¡çš„å­ä»»åŠ¡æ•°é‡
            self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))

            logger.info(f"ğŸ‰ Large PDF split complete: {len(chunks)} subtasks created for parent task {task_id}")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to split PDF: {e}")
            logger.warning("âš ï¸  Falling back to processing as single task")
            return False

    def _merge_parent_task_results(self, parent_task_id: str):
        """
        åˆå¹¶çˆ¶ä»»åŠ¡çš„æ‰€æœ‰å­ä»»åŠ¡ç»“æœ

        Args:
            parent_task_id: çˆ¶ä»»åŠ¡ID
        """
        try:
            # è·å–çˆ¶ä»»åŠ¡å’Œæ‰€æœ‰å­ä»»åŠ¡
            parent_task = self.task_db.get_task_with_children(parent_task_id)

            if not parent_task:
                raise ValueError(f"Parent task {parent_task_id} not found")

            children = parent_task.get("children", [])

            if not children:
                raise ValueError(f"No child tasks found for parent {parent_task_id}")

            # æŒ‰é¡µç æ’åºå­ä»»åŠ¡
            children.sort(key=lambda x: json.loads(x.get("options", "{}")).get("chunk_info", {}).get("start_page", 0))

            logger.info(f"ğŸ”€ Merging {len(children)} subtask results for parent task {parent_task_id}")

            # åˆ›å»ºçˆ¶ä»»åŠ¡è¾“å‡ºç›®å½•
            parent_output_dir = Path(self.output_dir) / Path(parent_task["file_path"]).stem
            parent_output_dir.mkdir(parents=True, exist_ok=True)

            # åˆå¹¶ Markdown
            markdown_parts = []
            json_pages = []
            has_json = False

            for idx, child in enumerate(children):
                if child["status"] != "completed":
                    logger.warning(f"âš ï¸  Child task {child['task_id']} not completed (status: {child['status']})")
                    continue

                result_dir = Path(child["result_path"])
                chunk_info = json.loads(child.get("options", "{}")).get("chunk_info", {})

                # è¯»å– Markdown
                md_files = list(result_dir.rglob("*.md"))
                if md_files:
                    md_file = None
                    for f in md_files:
                        if f.name == "result.md":
                            md_file = f
                            break
                    if not md_file:
                        md_file = md_files[0]

                    content = md_file.read_text(encoding="utf-8")

                    # æ·»åŠ åˆ†é¡µæ ‡è®°
                    if chunk_info:
                        markdown_parts.append(
                            f"\n\n<!-- Pages {chunk_info['start_page']}-{chunk_info['end_page']} -->\n\n"
                        )
                    markdown_parts.append(content)

                    logger.info(
                        f"   âœ… Merged chunk {idx+1}/{len(children)}: "
                        f"pages {chunk_info.get('start_page', '?')}-{chunk_info.get('end_page', '?')}"
                    )

                # è¯»å– JSON (å¦‚æœæœ‰)
                json_files = [
                    f
                    for f in result_dir.rglob("*.json")
                    if f.name in ["content.json", "result.json"] or "_content_list.json" in f.name
                ]

                if json_files:
                    try:
                        json_file = json_files[0]
                        json_content = json.loads(json_file.read_text(encoding="utf-8"))

                        # åˆå¹¶ JSON é¡µé¢æ•°æ®
                        if "pages" in json_content:
                            has_json = True
                            page_offset = chunk_info.get("start_page", 1) - 1

                            for page in json_content["pages"]:
                                # è°ƒæ•´é¡µç 
                                if "page_number" in page:
                                    page["page_number"] += page_offset
                                json_pages.append(page)
                    except Exception as json_e:
                        logger.warning(f"âš ï¸  Failed to merge JSON for chunk {idx+1}: {json_e}")

            # ä¿å­˜åˆå¹¶åçš„ Markdown
            merged_md = "".join(markdown_parts)
            md_output = parent_output_dir / "result.md"
            md_output.write_text(merged_md, encoding="utf-8")
            logger.info(f"ğŸ“„ Merged Markdown saved: {md_output}")

            # ä¿å­˜åˆå¹¶åçš„ JSON (å¦‚æœæœ‰)
            if has_json and json_pages:
                merged_json = {"pages": json_pages}
                json_output = parent_output_dir / "result.json"
                json_output.write_text(json.dumps(merged_json, indent=2, ensure_ascii=False), encoding="utf-8")
                logger.info(f"ğŸ“„ Merged JSON saved: {json_output}")

            # è§„èŒƒåŒ–è¾“å‡º
            normalize_output(parent_output_dir)

            # æ›´æ–°çˆ¶ä»»åŠ¡çŠ¶æ€
            self.task_db.update_task_status(
                task_id=parent_task_id, status="completed", result_path=str(parent_output_dir)
            )

            logger.info(f"âœ… Parent task {parent_task_id} merged successfully")

            # æ¸…ç†å­ä»»åŠ¡çš„ä¸´æ—¶æ–‡ä»¶
            self._cleanup_child_task_files(children)

        except Exception as e:
            logger.error(f"âŒ Failed to merge parent task {parent_task_id}: {e}")
            logger.exception(e)
            raise

    def _cleanup_child_task_files(self, children: list):
        """
        æ¸…ç†å­ä»»åŠ¡çš„ä¸´æ—¶æ–‡ä»¶

        Args:
            children: å­ä»»åŠ¡åˆ—è¡¨
        """
        try:
            for child in children:
                # åˆ é™¤å­ä»»åŠ¡çš„åˆ†ç‰‡ PDF æ–‡ä»¶
                if child.get("file_path"):
                    chunk_file = Path(child["file_path"])
                    if chunk_file.exists() and chunk_file.is_file():
                        try:
                            chunk_file.unlink()
                            logger.debug(f"ğŸ—‘ï¸  Deleted chunk file: {chunk_file.name}")
                        except Exception as e:
                            logger.warning(f"âš ï¸  Failed to delete chunk file {chunk_file.name}: {e}")

                # å¯é€‰: åˆ é™¤å­ä»»åŠ¡çš„ç»“æœç›®å½• (å¦‚æœéœ€è¦èŠ‚çœç©ºé—´)
                # æ³¨æ„: è¿™ä¼šåˆ é™¤ä¸­é—´ç»“æœ,å¯èƒ½å½±å“è°ƒè¯•
                # if child.get("result_path"):
                #     result_dir = Path(child["result_path"])
                #     if result_dir.exists() and result_dir.is_dir():
                #         try:
                #             shutil.rmtree(result_dir)
                #             logger.debug(f"ğŸ—‘ï¸  Deleted result dir: {result_dir.name}")
                #         except Exception as e:
                #             logger.warning(f"âš ï¸  Failed to delete result dir {result_dir.name}: {e}")

        except Exception as e:
            logger.warning(f"âš ï¸  Failed to cleanup child task files: {e}")

    def _process_with_format_engine(self, file_path: str, options: dict, engine_name: Optional[str] = None) -> dict:
        """
        ä½¿ç”¨æ ¼å¼å¼•æ“å¤„ç†ä¸“ä¸šé¢†åŸŸæ ¼å¼æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            options: å¤„ç†é€‰é¡¹
            engine_name: æŒ‡å®šçš„å¼•æ“åç§°ï¼ˆå¦‚ fasta, genbankï¼‰ï¼Œä¸º None æ—¶è‡ªåŠ¨é€‰æ‹©
        """
        # è·å–è¯­è¨€è®¾ç½®
        lang = options.get("language", "en")

        # æ ¹æ®æŒ‡å®šçš„å¼•æ“åç§°æˆ–æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¼•æ“
        if engine_name:
            # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†å¼•æ“
            engine = FormatEngineRegistry.get_engine(engine_name)
            if engine is None:
                raise ValueError(f"Format engine '{engine_name}' not found or not registered")

            # éªŒè¯æ–‡ä»¶æ˜¯å¦é€‚åˆè¯¥å¼•æ“
            if not engine.validate_file(file_path):
                raise ValueError(
                    f"File '{file_path}' is not supported by '{engine_name}' engine. "
                    f"Supported extensions: {', '.join(engine.SUPPORTED_EXTENSIONS)}"
                )

            # ä½¿ç”¨æŒ‡å®šå¼•æ“å¤„ç†
            result = engine.parse(file_path, options={"language": lang})
        else:
            # è‡ªåŠ¨é€‰æ‹©å¼•æ“ï¼ˆæ ¹æ®æ–‡ä»¶æ‰©å±•åï¼‰
            engine = FormatEngineRegistry.get_engine_by_extension(file_path)
            if engine is None:
                raise ValueError(f"No format engine available for file: {file_path}")

            result = engine.parse(file_path, options={"language": lang})

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸“å±è¾“å‡ºç›®å½•ï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´ï¼‰
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç»“æœï¼ˆä¸å…¶ä»–å¼•æ“ä¿æŒä¸€è‡´çš„å‘½åè§„èŒƒï¼‰
        # ä¸»ç»“æœæ–‡ä»¶ï¼šresult.md å’Œ result.json
        output_file = output_dir / "result.md"
        output_file.write_text(result["markdown"], encoding="utf-8")
        logger.info("ğŸ“„ Main result saved: result.md")

        # å¤‡ä»½æ–‡ä»¶ï¼šä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        backup_md_file = output_dir / f"{Path(file_path).stem}_{result['format']}.md"
        backup_md_file.write_text(result["markdown"], encoding="utf-8")
        logger.info(f"ğŸ“„ Backup saved: {backup_md_file.name}")

        # ä¹Ÿä¿å­˜ JSON ç»“æ„åŒ–æ•°æ®
        json_file = output_dir / "result.json"
        json_file.write_text(json.dumps(result["json_content"], indent=2, ensure_ascii=False), encoding="utf-8")
        logger.info("ğŸ“„ Main JSON saved: result.json")

        # å¤‡ä»½ JSON æ–‡ä»¶
        backup_json_file = output_dir / f"{Path(file_path).stem}_{result['format']}.json"
        backup_json_file.write_text(json.dumps(result["json_content"], indent=2, ensure_ascii=False), encoding="utf-8")
        logger.info(f"ğŸ“„ Backup JSON saved: {backup_json_file.name}")

        # è§„èŒƒåŒ–è¾“å‡ºï¼ˆç»Ÿä¸€æ–‡ä»¶åå’Œç›®å½•ç»“æ„ï¼‰
        # Format Engine å·²ç»è¾“å‡ºæ ‡å‡†æ ¼å¼ï¼Œä½†ä»ç„¶è°ƒç”¨è§„èŒƒåŒ–å™¨ä»¥ç¡®ä¿ä¸€è‡´æ€§
        normalize_output(output_dir)

        return {
            "result_path": str(output_dir),  # è¿”å›ä»»åŠ¡ä¸“å±ç›®å½•
            "content": result["content"],
            "json_path": str(json_file),
            "json_content": result["json_content"],
        }

    def decode_request(self, request):
        """
        è§£ç è¯·æ±‚: LitServe ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•æ¥è§£æè¯·æ±‚
        è¯·æ±‚æ ¼å¼: {"action": "health" | "poll"}
        """
        return request.get("action", "health")

    def predict(self, action):
        """
        å¤„ç†è¯·æ±‚
        Args:
            action: è¯·æ±‚åŠ¨ä½œ
                - "health": å¥åº·æ£€æŸ¥
                - "poll": æ‰‹åŠ¨æ‹‰å–ä»»åŠ¡ï¼ˆå½“ worker loop ç¦ç”¨æ—¶ï¼‰
        Returns: å“åº”å­—å…¸
        """
        if action == "health":
            # å¥åº·æ£€æŸ¥
            vram_gb = None
            if "cuda" in str(self.device).lower():
                try:
                    vram_gb = get_vram(self.device.split(":")[-1])
                except Exception:
                    pass

            return {
                "status": "healthy",
                "worker_id": self.worker_id,
                "device": str(self.device),
                "vram_gb": vram_gb,
                "running": self.running,
                "current_task": self.current_task_id,
                "worker_loop_enabled": self.enable_worker_loop,
            }

        elif action == "poll":
            # æ‰‹åŠ¨æ‹‰å–ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•æˆ–ç¦ç”¨ worker loop æ—¶ï¼‰
            if self.enable_worker_loop:
                return {
                    "status": "skipped",
                    "message": "Worker is in auto-loop mode, manual polling is disabled",
                    "worker_id": self.worker_id,
                }

            task = self.task_db.pull_task()
            if task:
                task_id = task["task_id"]
                logger.info(f"ğŸ“¥ {self.worker_id} manually pulled task: {task_id}")

                try:
                    self._process_task(task)
                    logger.info(f"âœ… {self.worker_id} completed task: {task_id}")

                    return {"status": "completed", "task_id": task["task_id"], "worker_id": self.worker_id}
                except Exception as e:
                    return {
                        "status": "failed",
                        "task_id": task["task_id"],
                        "error": str(e),
                        "worker_id": self.worker_id,
                    }
            else:
                # Worker å¾ªç¯æ¨¡å¼ï¼šè¿”å›çŠ¶æ€ä¿¡æ¯
                return {
                    "status": "auto_mode",
                    "message": "Worker is running in auto-loop mode, tasks are processed automatically",
                    "worker_id": self.worker_id,
                    "worker_running": self.running,
                }

        else:
            return {
                "status": "error",
                "message": f'Invalid action: {action}. Use "health" or "poll".',
                "worker_id": self.worker_id,
            }

    def encode_response(self, response):
        """ç¼–ç å“åº”"""
        return response

    def teardown(self):
        """æ¸…ç†èµ„æºï¼ˆWorker å…³é—­æ—¶è°ƒç”¨ï¼‰"""
        # è·å– worker_idï¼ˆå¯èƒ½åœ¨ setup å¤±è´¥æ—¶æœªåˆå§‹åŒ–ï¼‰
        worker_id = getattr(self, "worker_id", "unknown")

        logger.info(f"ğŸ›‘ Worker {worker_id} shutting down...")

        # è®¾ç½® running æ ‡å¿—ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        if hasattr(self, "running"):
            self.running = False

        # ç­‰å¾… worker çº¿ç¨‹ç»“æŸ
        if hasattr(self, "worker_thread") and self.worker_thread.is_alive():
            self.worker_thread.join(timeout=5)

        logger.info(f"âœ… Worker {worker_id} stopped")


def start_litserve_workers(
    output_dir=None,  # é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    accelerator="auto",
    devices="auto",
    workers_per_device=1,
    port=8001,
    poll_interval=0.5,
    enable_worker_loop=True,
    paddleocr_vl_vllm_engine_enabled=False,
    paddleocr_vl_vllm_api_list=[],
):
    """
    å¯åŠ¨ LitServe Worker Pool

    Args:
        output_dir: è¾“å‡ºç›®å½•
        accelerator: åŠ é€Ÿå™¨ç±»å‹ (auto/cuda/cpu/mps)
        devices: ä½¿ç”¨çš„è®¾å¤‡ (auto/[0,1,2])
        workers_per_device: æ¯ä¸ª GPU çš„ worker æ•°é‡
        port: æœåŠ¡ç«¯å£
        poll_interval: Worker æ‹‰å–ä»»åŠ¡çš„é—´éš”ï¼ˆç§’ï¼‰
        enable_worker_loop: æ˜¯å¦å¯ç”¨ worker è‡ªåŠ¨å¾ªç¯æ‹‰å–ä»»åŠ¡
        paddleocr_vl_vllm_engine_enabled: æ˜¯å¦å¯ç”¨ PaddleOCR VL VLLM å¼•æ“
        paddleocr_vl_vllm_api_list: PaddleOCR VL VLLM API åˆ—è¡¨
    """

    logger.info("=" * 60)
    logger.success("ğŸš€ Starting MinerU Tianshu LitServe Worker Pool Final Config: ")
    logger.info(f"      ğŸ“‚ Output Directory: {output_dir}")
    logger.info(f"      ğŸ’¾ Devices: {devices}")
    logger.info(f"      ğŸ‘· Workers per Device: {workers_per_device}")
    logger.info(f"      ğŸ”Œ Port: {port}")
    logger.info(f"      ğŸ”„ Worker Loop: {'Enabled' if enable_worker_loop else 'Disabled'}")
    if enable_worker_loop:
        logger.info(f"      â±ï¸  Poll Interval: {poll_interval}s")
    logger.info(f"      ğŸ® Initial Accelerator setting: {accelerator}")
    logger.info("=" * 60)
    if paddleocr_vl_vllm_engine_enabled:
        if not paddleocr_vl_vllm_api_list:
            logger.error(
                "è¯·é…ç½® --paddleocr-vl-vllm-api-list å‚æ•°ï¼Œæˆ–ç§»é™¤ --paddleocr-vl-vllm-engine-enabled ä»¥ç¦ç”¨ PaddleOCR VL VLLM å¼•æ“"
            )
            sys.exit(1)
        logger.success(f"PaddleOCR VL VLLM å¼•æ“å·²å¯ç”¨ï¼ŒAPI åˆ—è¡¨ä¸º: {paddleocr_vl_vllm_api_list}")
    else:
        os.environ.pop("PADDLEOCR_VL_VLLM_ENABLED", None)
        logger.info("PaddleOCR VL VLLM å¼•æ“å·²ç¦ç”¨")

    # 1. å®ä¾‹åŒ– API æ—¶ä¼ å…¥æ•°æ®
    api = MinerUWorkerAPI(
        output_dir=output_dir,
        poll_interval=poll_interval,
        enable_worker_loop=enable_worker_loop,
        paddleocr_vl_vllm_engine_enabled=paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=paddleocr_vl_vllm_api_list,
    )

    server = ls.LitServer(
        api,
        accelerator=accelerator,
        devices=devices,
        workers_per_device=workers_per_device,
        timeout=False,  # ä¸è®¾ç½®è¶…æ—¶
    )

    def graceful_shutdown(signum=None, frame=None):
        """
        ä¼˜é›…å…³é—­å¤„ç†å™¨: å¤„ç†å…³é—­ä¿¡å·ï¼Œä¼˜é›…åœ°åœæ­¢ worker
        note:å®é™…çš„ worker å®ä¾‹ç”± LitServe ç®¡ç†, teardown ä¼šåœ¨æ¯ä¸ª worker è¿›ç¨‹ä¸­è¢«è°ƒç”¨
        """
        logger.info("ğŸ›‘ Received shutdown signal, gracefully stopping workers...")
        if hasattr(api, "teardown"):
            api.teardown()
        sys.exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆCtrl+C ç­‰ï¼‰
    signal.signal(signal.SIGINT, graceful_shutdown)
    signal.signal(signal.SIGTERM, graceful_shutdown)

    # æ³¨å†Œ atexit å¤„ç†å™¨ï¼ˆæ­£å¸¸é€€å‡ºæ—¶è°ƒç”¨ï¼‰
    atexit.register(lambda: api.teardown() if hasattr(api, "teardown") else None)

    logger.info("âœ… LitServe worker pool initialized")
    logger.info(f"ğŸ“¡ Listening on: http://0.0.0.0:{port}/predict")
    if enable_worker_loop:
        logger.info("ğŸ” Workers will continuously poll and process tasks")
    else:
        logger.info("ğŸ”„ Workers will wait for scheduler triggers")
    logger.info("=" * 60)

    # å¯åŠ¨æœåŠ¡å™¨
    # æ³¨æ„ï¼šLitServe å†…ç½® MCP å·²é€šè¿‡ monkeypatch å®Œå…¨ç¦ç”¨ï¼ˆæˆ‘ä»¬æœ‰ç‹¬ç«‹çš„ MCP Serverï¼‰
    server.run(port=port, generate_client_file=False)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MinerU Tianshu LitServe Worker Pool")
    parser.add_argument(
        "--output-dir",
        type=str,
        default=os.getenv("OUTPUT_PATH", "./app_data/mineru_tianshu_output"),
        help="è¾“å‡ºç›®å½• (é»˜è®¤: backend/app_data/mineru_tianshu_output)",
    )
    parser.add_argument("--port", type=int, default=8001, help="Server port (default: 8001, or from WORKER_PORT env)")
    parser.add_argument(
        "--accelerator",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="Accelerator type (default: auto)",
    )
    parser.add_argument("--workers-per-device", type=int, default=1, help="Number of workers per device (default: 1)")
    parser.add_argument("--devices", type=str, default="auto", help="Devices to use, comma-separated (default: auto)")
    parser.add_argument(
        "--poll-interval", type=float, default=0.5, help="Worker poll interval in seconds (default: 0.5)"
    )
    parser.add_argument(
        "--disable-worker-loop",
        action="store_true",
        help="Disable automatic worker loop (workers will wait for manual triggers)",
    )
    parser.add_argument(
        "--paddleocr-vl-vllm-engine-enabled",
        action="store_true",
        default=False,
        help="æ˜¯å¦å¯ç”¨ PaddleOCR VL VLLM å¼•æ“ (é»˜è®¤: False)",
    )
    parser.add_argument(
        "--paddleocr-vl-vllm-api-list",
        type=parse_list_arg,
        default=[],
        help='PaddleOCR VL VLLM API åˆ—è¡¨ï¼ˆPython list å­—é¢é‡æ ¼å¼ï¼Œå¦‚: \'["http://127.0.0.1:8000/v1", "http://127.0.0.1:8001/v1"]\'ï¼‰',
    )
    args = parser.parse_args()

    # ============================================================================
    # é…ç½®ä¼˜å…ˆçº§: å‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
    # ============================================================================
    # å¤„ç† port é…ç½®
    if args.port == 8001:  # ä½¿ç”¨é»˜è®¤å€¼æ—¶ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        env_port = os.getenv("WORKER_PORT")
        if env_port:
            try:
                args.port = int(env_port)
            except ValueError:
                logger.warning(f"âš ï¸  Invalid WORKER_PORT={env_port}, using default: 8001")

    # å¤„ç† workers_per_device é…ç½®
    if args.workers_per_device == 1:  # ä½¿ç”¨é»˜è®¤å€¼æ—¶ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        env_workers = os.getenv("WORKER_GPUS")
        if env_workers:
            try:
                args.workers_per_device = int(env_workers)
            except ValueError:
                logger.warning(f"âš ï¸  Invalid WORKER_GPUS={env_workers}, using default: 1")

    # å¤„ç† devices é…ç½®
    if args.devices == "auto":
        env_devices = os.getenv("CUDA_VISIBLE_DEVICES")  # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES è¯»å–
        if env_devices:
            args.devices = env_devices
        else:
            # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ CUDA è®¾å¤‡
            try:
                import torch

                if torch.cuda.is_available():
                    device_count = torch.cuda.device_count()
                    args.devices = ",".join(str(i) for i in range(device_count))
                else:
                    args.devices = "auto"  # ä¿æŒ autoï¼Œè®© LitServe ä½¿ç”¨ CPU
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to detect CUDA devices: {e}")
                args.devices = "auto"

    # è§£æ devicesï¼ˆæ”¯æŒé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
    if args.devices != "auto":
        try:
            args.devices = [int(d.strip()) for d in args.devices.split(",")]
        except ValueError:
            logger.error(f"âŒ Invalid devices format: {args.devices}. Use comma-separated integers (e.g., '0,1,2')")
            sys.exit(1)

    # å¤„ç† output_dir é…ç½®
    if args.output_dir is None:
        args.output_dir = os.getenv("OUTPUT_PATH", "./app_data/mineru_tianshu_output")

    # å¤„ç† accelerator é…ç½®
    if args.accelerator == "auto":
        args.accelerator = resolve_auto_accelerator()  # æ‰‹åŠ¨è§£æacceleratorçš„å…·ä½“è®¾ç½®
        logger.info(f"ğŸ’« Auto-resolved Accelerator: {args.accelerator}")

    # å¯åŠ¨ LitServe å·¥ä½œè¿›ç¨‹
    start_litserve_workers(
        output_dir=args.output_dir,
        accelerator=args.accelerator,
        devices=args.devices,
        workers_per_device=args.workers_per_device,
        port=args.port,
        poll_interval=args.poll_interval,
        enable_worker_loop=not args.disable_worker_loop,
        paddleocr_vl_vllm_engine_enabled=args.paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=args.paddleocr_vl_vllm_api_list,
    )
