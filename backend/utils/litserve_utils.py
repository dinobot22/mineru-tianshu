import os
import sys
import warnings
import importlib.util
from contextlib import asynccontextmanager
from pathlib import Path
from loguru import logger
from litserve.connector import check_cuda_with_nvidia_smi


def apply_litserve_patch():
    """
    ä¿®å¤ litserve MCP ä¸ mcp>=1.1.0 ä¸å…¼å®¹é—®é¢˜
    å®Œå…¨ç¦ç”¨ LitServe çš„å†…éƒ¨ MCP ä»¥é¿å…ä¸æˆ‘ä»¬çš„ç‹¬ç«‹ MCP Server å†²çª
    """
    try:
        # Patch LitServe's MCP module to disable it completely
        import litserve.mcp as ls_mcp

        # Inject MCPServer (mcp.server.lowlevel.Server) as dummy
        if not hasattr(ls_mcp, "MCPServer"):

            class DummyMCPServer:
                def __init__(self, *args, **kwargs):
                    pass

            ls_mcp.MCPServer = DummyMCPServer
            if "litserve.mcp" in sys.modules:
                sys.modules["litserve.mcp"].MCPServer = DummyMCPServer

        # Inject StreamableHTTPSessionManager as dummy
        if not hasattr(ls_mcp, "StreamableHTTPSessionManager"):

            class DummyStreamableHTTPSessionManager:
                def __init__(self, *args, **kwargs):
                    pass

            ls_mcp.StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager
            if "litserve.mcp" in sys.modules:
                sys.modules["litserve.mcp"].StreamableHTTPSessionManager = DummyStreamableHTTPSessionManager

        # Replace _LitMCPServerConnector with a complete dummy implementation
        class DummyMCPConnector:
            """å®Œå…¨ç¦ç”¨ LitServe å†…ç½® MCP çš„ Dummy å®ç°"""

            def __init__(self, *args, **kwargs):
                self.mcp_server = None
                self.session_manager = None
                self.request_handler = None

            @asynccontextmanager
            async def lifespan(self, app):
                """ç©ºçš„ lifespan context managerï¼Œä¸åšä»»ä½•äº‹æƒ…"""
                yield  # ä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è®©æœåŠ¡å™¨å¯åŠ¨

            def connect_mcp_server(self, *args, **kwargs):
                """ç©ºçš„ connect_mcp_server æ–¹æ³•ï¼Œä¸åšä»»ä½•äº‹æƒ…"""
                pass  # ä»€ä¹ˆéƒ½ä¸åšï¼Œè·³è¿‡ MCP åˆå§‹åŒ–

        # æ›¿æ¢ _LitMCPServerConnector ç±»
        ls_mcp._LitMCPServerConnector = DummyMCPConnector

        # åŒæ—¶æ›´æ–° sys.modules ä¸­çš„å¼•ç”¨
        if "litserve.mcp" in sys.modules:
            sys.modules["litserve.mcp"]._LitMCPServerConnector = DummyMCPConnector

    except Exception as e:
        # If patching fails, log warning and continue
        # The server might still work or fail with a clearer error message
        warnings.warn(f"Failed to patch litserve.mcp (MCP will be disabled): {e}")


def verify_pytorch_cuda():
    """
    éªŒè¯ PyTorch CUDA è®¾ç½®ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ
    """
    import os
    import torch
    from loguru import logger

    try:
        if torch.cuda.is_available():
            visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "all")
            device_count = torch.cuda.device_count()
            logger.info("âœ… PyTorch CUDA verified:")
            logger.info(f"   CUDA_VISIBLE_DEVICES = {visible_devices}")
            logger.info(f"   torch.cuda.device_count() = {device_count}")
            if device_count == 1:
                logger.info(f"   âœ… SUCCESS: Process isolated to 1 GPU (physical GPU {visible_devices})")
            else:
                logger.warning(f"   âš ï¸  WARNING: Expected 1 GPU but found {device_count}")
        else:
            logger.warning("âš ï¸  CUDA not available")
    except Exception as e:
        logger.warning(f"âš ï¸  Failed to verify PyTorch CUDA: {e}")


def init_task_db(TaskDB, db_path_env):
    """
    åˆå§‹åŒ–ä»»åŠ¡æ•°æ®åº“ï¼Œè¿”å› TaskDB å®ä¾‹
    """
    # åˆå§‹åŒ–ä»»åŠ¡æ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå…¼å®¹ Docker å’Œæœ¬åœ°ï¼‰
    db_path = Path(db_path_env).resolve()
    db_path.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ“Š DATABASE_PATH={db_path_env} â†’ resolved={db_path}")
    task_db = TaskDB(str(db_path))  # å¦‚ TaskDB åªæ¥å— str

    # éªŒè¯æ•°æ®åº“è¿æ¥å¹¶è¾“å‡ºåˆå§‹ç»Ÿè®¡
    try:
        stats = task_db.get_queue_stats()
        logger.info(f"ğŸ“Š Database initialized: {db_path} (exists: {db_path.exists()})")
        logger.info(f"ğŸ“Š TaskDB.db_path: {task_db.db_path}")
        logger.info(f"ğŸ“Š Initial queue stats: {stats}")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize database or get stats: {e}")
        logger.exception(e)

    return task_db


def resolve_auto_accelerator():
    """
    å½“ accelerator è®¾ç½®ä¸º "auto" æ—¶ï¼Œä½¿ç”¨å…ƒæ•°æ®åŠç¯å¢ƒä¿¡æ¯è‡ªåŠ¨æ£€æµ‹æœ€åˆé€‚çš„åŠ é€Ÿå™¨ç±»å‹(ä¸ç›´æ¥å¯¼å…¥torch)
    Return: str  æ£€æµ‹åˆ°çš„åŠ é€Ÿå™¨ç±»å‹ ("cuda" æˆ– "cpu")
    """
    try:
        from importlib.metadata import distribution

        distribution("torch")
        torch_is_installed = True
    except Exception as e:
        torch_is_installed = False
        logger.warning(f"Torch is not installed or cannot be imported: {e}")

    if torch_is_installed and check_cuda_with_nvidia_smi() > 0:
        return "cuda"
    return "cpu"


def configure_model_source(model_source="auto"):
    """
    é…ç½®æ¨¡å‹ä¸‹è½½æºï¼ˆå¿…é¡»åœ¨ MinerU åˆå§‹åŒ–ä¹‹å‰ï¼‰
    ä»ç¯å¢ƒå˜é‡ MODEL_DOWNLOAD_SOURCE è¯»å–é…ç½®
    æ”¯æŒ: modelscope, huggingface, auto (é»˜è®¤)
    """
    # é¿å…é‡å¤é…ç½®/æ—¥å¿— (åœ¨å¤šè¿›ç¨‹ Worker ä¸­ï¼Œç¯å¢ƒå˜é‡ä¼šè¢«ç»§æ‰¿)
    if os.environ.get("MINERU_MODEL_SOURCE_CONFIGURED") == "1":
        return
    # è§£æ auto æ¨¡å¼
    if model_source == "auto":
        if importlib.util.find_spec("modelscope") is not None:
            model_source = "modelscope"
        else:
            model_source = "huggingface"

    if model_source == "modelscope":
        # å°è¯•ä½¿ç”¨ ModelScopeï¼ˆä¼˜å…ˆï¼‰
        try:
            if importlib.util.find_spec("modelscope") is not None:
                os.environ["MINERU_MODEL_SOURCE"] = "modelscope"
                logger.info("ğŸ“¦ Model download source: ModelScope (å›½å†…æ¨è)")
                logger.info("   Note: ModelScope automatically uses China mirror for faster downloads")
            else:
                raise ImportError("modelscope not found")
        except ImportError:
            if model_source == "modelscope":
                logger.warning("âš ï¸  ModelScope not available, falling back to HuggingFace")
            model_source = "huggingface"

    if model_source == "huggingface":
        # é…ç½® HuggingFace é•œåƒï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä½¿ç”¨å›½å†…é•œåƒï¼‰
        hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
        os.environ.setdefault("HF_ENDPOINT", hf_endpoint)
        logger.info(f"ğŸ“¦ Model download source: HuggingFace (via: {hf_endpoint})")
    elif model_source != "modelscope":
        logger.warning(f"âš ï¸  Unknown model download source: {model_source}")
    # æ ‡è®°å·²é…ç½®
    os.environ["MINERU_MODEL_SOURCE_CONFIGURED"] = "1"
